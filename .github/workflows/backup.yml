name: Backup Database and Storage

on:
  workflow_dispatch:  # Allow manual triggers
    inputs:
      dispatch_id:
        description: 'Unique dispatch ID for tracking this backup'
        required: false
        type: string
      trigger_type:
        description: 'Type of trigger (manual or scheduled)'
        required: false
        type: string
        default: 'manual'
  schedule:
    - cron: '0 2 * * *'  # Daily at 2:00 AM UTC

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    # ‚úÖ FIX: Define all environment variables at job level
    # This makes them available to ALL steps
    # Note: Using VITE_ prefixed secrets to match current GitHub Secrets configuration
    env:
      SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
      SUPABASE_DB_URL: ${{ secrets.DATABASE_URL }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_S3_REGION: ${{ secrets.AWS_S3_REGION }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SUPABASE_BUCKETS_TO_BACKUP: ${{ secrets.SUPABASE_BUCKETS_TO_BACKUP }}
      DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
      TRIGGER_TYPE: ${{ github.event.inputs.trigger_type || 'scheduled' }}
      BACKUP_S3_FILES: ${{ secrets.BACKUP_S3_FILES || 'true' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        # ‚úÖ FIX: Disable submodule checkout to avoid the warning
        with:
          submodules: false

      - name: Debug environment variables
        # ‚úÖ DEBUG: Check if secrets are available (without exposing values)
        run: |
          echo "Checking environment variables..."
          echo "SUPABASE_URL: ${SUPABASE_URL:+SET (hidden)}${SUPABASE_URL:-MISSING}"
          echo "SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY:+SET (hidden)}${SUPABASE_SERVICE_ROLE_KEY:-MISSING}"
          echo "SUPABASE_DB_URL: ${SUPABASE_DB_URL:+SET (hidden)}${SUPABASE_DB_URL:-MISSING}"
          echo "AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:+SET (hidden)}${AWS_ACCESS_KEY_ID:-MISSING}"
          echo "AWS_S3_BUCKET: ${AWS_S3_BUCKET:+SET (hidden)}${AWS_S3_BUCKET:-MISSING}"
          echo "DISPATCH_ID: $DISPATCH_ID"
          
          # Check if secrets are actually set
          if [ -z "$SUPABASE_URL" ]; then
            echo "‚ùå ERROR: VITE_SUPABASE_URL secret is not set in GitHub Secrets"
            echo "Please go to: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo "And add VITE_SUPABASE_URL secret"
            exit 1
          fi
          
          if [ -z "$SUPABASE_SERVICE_ROLE_KEY" ]; then
            echo "‚ùå ERROR: VITE_SUPABASE_SERVICE_ROLE_KEY secret is not set in GitHub Secrets"
            echo "Please go to: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo "And add VITE_SUPABASE_SERVICE_ROLE_KEY secret"
            exit 1
          fi
          
          echo "‚úì All required secrets are set"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Check backup enable status
        # ‚úÖ FIX: Environment variables are inherited from job-level env:
        run: |
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          
          // ‚úÖ FIX: Get from process.env (automatically available from job env:)
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          // ‚úÖ DEBUG: Validate URL format (without exposing full value)
          console.log('Validating Supabase URL...');
          console.log('URL length:', supabaseUrl ? supabaseUrl.length : 0);
          console.log('URL starts with https://', supabaseUrl ? supabaseUrl.startsWith('https://') : false);
          console.log('URL preview:', supabaseUrl ? supabaseUrl.substring(0, 30) + '...' : 'MISSING');
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
            console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value starts with:', supabaseUrl.substring(0, 20));
            console.error('Expected format: https://xxxxx.supabase.co');
            console.error('');
            console.error('Please check your VITE_SUPABASE_URL secret in GitHub Settings');
            console.error('It should be your Supabase project URL, not the database connection string');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Remove trailing slash if present
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          console.log('‚úì Supabase credentials found');
          console.log('‚úì URL format validated');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('system_settings_kv')
            .select('value')
            .eq('key', 'backup_enabled')
            .single()
            .then(({ data, error }) => {
              if (error && error.code !== 'PGRST116') {
                console.error('‚úó Error checking backup status:', error);
                process.exit(1);
              }
              
              const enabled = data?.value?.enabled ?? false;
              if (!enabled) {
                console.log('‚Ñπ Backup is disabled. Exiting.');
                process.exit(0);
              }
              
              console.log('‚úì Backup is enabled. Proceeding...');
            })
            .catch((err) => {
              console.error('‚úó Failed to check backup status:', err);
              process.exit(1);
            });
          "

      - name: Update backup status (in progress)
        run: |
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const dispatchId = process.env.DISPATCH_ID;
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          if (!dispatchId) {
            console.error('‚úó Missing DISPATCH_ID');
            process.exit(1);
          }
          
          console.log('‚úì Updating backup status to in_progress');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('backup_history')
            .update({ status: 'in_progress' })
            .eq('dispatch_id', dispatchId)
            .then(({ error }) => {
              if (error) {
                console.error('‚úó Error updating backup status:', error);
                process.exit(1);
              }
              console.log('‚úì Backup status updated to in_progress');
            })
            .catch((err) => {
              console.error('‚úó Failed to update backup status:', err);
              process.exit(1);
            });
          "

      - name: Export database
        continue-on-error: false  # ‚úÖ Fail if database export fails (critical step)
        run: |
          mkdir -p backup/db
          
          # ‚úÖ FIX: Verify connection string is set
          if [ -z "$SUPABASE_DB_URL" ]; then
            echo "‚ùå ERROR: DATABASE_URL secret is not set"
            echo "Please set DATABASE_URL secret in GitHub Settings"
            echo "Format: postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres"
            exit 1
          fi
          
          echo "‚úì Starting database export..."
          echo "Connection string format: ${SUPABASE_DB_URL%%@*}@***" # Show only user@host part
          
          # ‚úÖ FIX: Try pg_dump, handle network errors gracefully
          pg_dump "$SUPABASE_DB_URL" \
            --no-owner \
            --no-privileges \
            --format=plain \
            --blobs \
            --verbose \
            --file=backup/db/backup.sql \
            2>&1 | tee /tmp/pg_dump_output.log || {
              PG_DUMP_ERROR=$(cat /tmp/pg_dump_output.log)
              
              # Check if it's a network/unreachable error (Supabase free plan issue)
              if echo "$PG_DUMP_ERROR" | grep -q "Network is unreachable\|Connection refused\|timeout\|unreachable"; then
                echo ""
                echo "‚ö†Ô∏è WARNING: Direct database connection failed (Network unreachable)"
                echo "This is common with Supabase Free Plan - external connections are blocked"
                echo ""
                echo "üîÑ Attempting backup via Supabase Edge Function (bypasses IP restrictions)..."
                echo ""
                
                # ‚úÖ FIX: Use Edge Function to export database via API
                node -e "
                const { createClient } = require('@supabase/supabase-js');
                const fs = require('fs');
                
                const supabaseUrl = process.env.SUPABASE_URL;
                const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
                
                if (!supabaseUrl || !supabaseKey) {
                  console.error('‚úó Cannot use Edge Function export - missing credentials');
                  process.exit(1);
                }
                
                if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
                  console.error('‚úó Invalid SUPABASE_URL format');
                  process.exit(1);
                }
                
                const cleanUrl = supabaseUrl.replace(/\/$/, '');
                const supabase = createClient(cleanUrl, supabaseKey);
                
                console.log('‚úì Getting admin user for Edge Function authentication...');
                
                // ‚úÖ FIX: Get a valid admin user_id from system_users
                const { data: adminUsers, error: adminError } = await supabase
                  .from('system_users')
                  .select('user_id, email, status, role_id')
                  .eq('status', 'active')
                  .limit(1);
                
                let userId = null;
                
                if (!adminError && adminUsers && adminUsers.length > 0) {
                  // Try to find a user with admin role
                  const { data: roles } = await supabase
                    .from('roles')
                    .select('role_id, role_name, permissions');
                  
                  if (roles) {
                    const adminRoleIds = roles
                      .filter(r => {
                        const perms = (r.permissions || '').toLowerCase();
                        const name = (r.role_name || '').toLowerCase();
                        return perms === 'all' || name.includes('admin') || name.includes('super');
                      })
                      .map(r => r.role_id);
                    
                    const adminUser = adminUsers.find(u => adminRoleIds.includes(u.role_id));
                    if (adminUser) {
                      userId = adminUser.user_id;
                      console.log('‚úì Found admin user:', adminUser.email);
                    } else {
                      userId = adminUsers[0].user_id;
                      console.log('‚úì Using active user:', adminUsers[0].email);
                    }
                  } else {
                    userId = adminUsers[0].user_id;
                    console.log('‚úì Using active user:', adminUsers[0].email);
                  }
                }
                
                if (!userId) {
                  throw new Error('No active admin user found in system_users table. Cannot authenticate Edge Function.');
                }
                
                console.log('‚úì Calling export-database Edge Function...');
                console.log('  Note: This uses Supabase API (no IP restrictions)');
                
                // Call the Edge Function using fetch (more reliable in GitHub Actions)
                const functionUrl = cleanUrl + '/functions/v1/export-database';
                console.log('  ‚Üí Calling Edge Function:', functionUrl);
                console.log('  ‚Üí User ID:', userId);
                console.log('  ‚Üí Supabase URL:', cleanUrl);
                
                // First, verify the Edge Function exists by checking if it's accessible
                try {
                  const healthCheck = await fetch(functionUrl, {
                    method: 'OPTIONS',
                    headers: {
                      'Authorization': 'Bearer ' + supabaseKey,
                      'apikey': supabaseKey,
                    }
                  });
                  if (healthCheck.status === 404) {
                    throw new Error('Edge Function "export-database" not found. Please deploy it using: supabase functions deploy export-database --no-verify-jwt');
                  }
                } catch (healthErr) {
                  if (healthErr.message.includes('not found')) {
                    throw healthErr;
                  }
                  // Ignore other health check errors, proceed with actual call
                }
                
                const response = await fetch(functionUrl, {
                  method: 'POST',
                  headers: {
                    'Content-Type': 'application/json',
                    'Authorization': 'Bearer ' + supabaseKey,
                    'apikey': supabaseKey,
                  },
                  body: JSON.stringify({ user_id: userId })
                });
                
                console.log('  ‚Üí Edge Function response status:', response.status);
                
                if (!response.ok) {
                  const errorText = await response.text();
                  console.error('  ‚úó Edge Function error response:', errorText);
                  throw new Error('Edge Function returned ' + response.status + ': ' + errorText);
                }
                
                const result = await response.json();
                console.log('  ‚Üí Edge Function result:', JSON.stringify({
                  success: result.success,
                  tables_exported: result.tables_exported,
                  tables_total: result.tables_total,
                  sql_size: result.sql_size,
                  message: result.message
                }));
                
                if (result.error || !result.success) {
                  throw new Error(result.error?.message || result.message || 'Export failed');
                }
                
                if (!result.sql_base64) {
                  throw new Error('Edge Function returned no SQL data');
                }
                
                // Decode base64 SQL
                const sql = Buffer.from(result.sql_base64, 'base64').toString('utf-8');
                
                if (!sql || sql.length === 0) {
                  throw new Error('Decoded SQL is empty');
                }
                
                // Write to file
                fs.writeFileSync('backup/db/backup.sql', sql);
                
                console.log('‚úì Database exported via Edge Function');
                console.log('  - Tables exported:', result.tables_exported, '/', result.tables_total || '?');
                console.log('  - SQL size:', (result.sql_size / 1024).toFixed(2), 'KB');
                console.log('  - SQL file size:', (sql.length / 1024).toFixed(2), 'KB');
                console.log('  - Message:', result.message || 'Export completed');
                console.log('‚ö†Ô∏è Note: This is a data-only export (INSERT statements)');
                console.log('‚ö†Ô∏è Schema (CREATE TABLE, functions, triggers) is NOT included');
                console.log('‚ö†Ô∏è For complete backup with schema, use pg_dump (requires Pro plan or IP whitelisting)');
                ")
                .catch((err) => {
                  console.error('‚úó Failed to export via Edge Function:', err.message);
                  console.error('');
                  console.error('üìã Alternative solutions:');
                  console.error('1. Enable IP whitelisting in Supabase Dashboard');
                  console.error('2. Upgrade to Supabase Pro Plan');
                  console.error('3. Use Supabase Dashboard backup feature');
                  console.error('');
                  console.error('Creating backup note file...');
                  
                  // Create note file
                  const note = '-- ‚ö†Ô∏è DATABASE BACKUP FAILED - Network Connection Issue\n' +
                    '-- \n' +
                    '-- Error: Network is unreachable\n' +
                    '-- Cause: Supabase Free Plan blocks external database connections\n' +
                    '-- \n' +
                    '-- Attempted solutions:\n' +
                    '-- 1. Direct pg_dump connection: FAILED (network blocked)\n' +
                    '-- 2. Edge Function export: FAILED (' + err.message + ')\n' +
                    '-- \n' +
                    '-- Solutions:\n' +
                    '-- 1. Enable IP whitelisting in Supabase Dashboard:\n' +
                    '--    Settings ‚Üí Database ‚Üí Network Restrictions ‚Üí Add IPs\n' +
                    '--    (May not be available on free plan)\n' +
                    '-- \n' +
                    '-- 2. Upgrade to Supabase Pro Plan ($25/month):\n' +
                    '--    Includes IP whitelisting and external connection support\n' +
                    '-- \n' +
                    '-- 3. Use Supabase Dashboard backup (if available):\n' +
                    '--    Dashboard ‚Üí Database ‚Üí Backups ‚Üí Create backup\n' +
                    '-- \n' +
                    '-- This backup file is empty because database export could not be completed.\n' +
                    '-- Other backups (auth users, storage files) may still be available.\n' +
                    '-- \n' +
                    '-- For production use, upgrade to Pro Plan for reliable backups.\n';
                  
                  fs.writeFileSync('backup/db/backup.sql', note);
                  console.error('‚úì Created backup note file');
                  process.exit(0); // Don't fail the workflow
                });
                "
                
                if [ $? -eq 0 ] && [ -f backup/db/backup.sql ] && [ -s backup/db/backup.sql ]; then
                  echo "‚úì Database backup completed via Edge Function"
                  echo "‚úì File size: $(du -h backup/db/backup.sql | cut -f1)"
                  echo "‚úì Line count: $(wc -l < backup/db/backup.sql) lines"
                else
                  echo "‚ùå Database backup via Edge Function FAILED"
                  echo "‚ùå This is a critical error - database backup is required"
                  if [ -f backup/db/backup.sql ]; then
                    echo "‚ö†Ô∏è File exists but is empty or invalid"
                    echo "‚ö†Ô∏è File size: $(stat -f%z backup/db/backup.sql 2>/dev/null || stat -c%s backup/db/backup.sql 2>/dev/null || echo 'unknown') bytes"
                  else
                    echo "‚ö†Ô∏è Backup file was not created"
                  fi
                  exit 1
                fi
              else
                echo "‚ùå pg_dump failed with error:"
                echo "$PG_DUMP_ERROR"
                echo ""
                echo "Common issues:"
                echo "1. Connection string format incorrect"
                echo "2. Database password incorrect"
                echo "3. Using pooled connection (port 6543) instead of direct (port 5432)"
                echo "4. IP not whitelisted (Supabase free plan may require IP whitelisting)"
                echo ""
                echo "Verify SUPABASE_DB_URL format:"
                echo "Correct: postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres"
                echo "Wrong:   postgresql://postgres:password@aws-1-eu-west-1.pooler.supabase.com:6543/postgres"
                # Exit with error for other issues (not network)
                exit 1
              fi
            }
          
          if [ -f backup/db/backup.sql ] && [ -s backup/db/backup.sql ]; then
            echo "‚úì Database export completed successfully"
          else
            echo "‚ö†Ô∏è Database export file is empty or missing"
          fi

      - name: Export auth users and system_users
        continue-on-error: true  # ‚úÖ Continue even if auth export fails (app uses custom auth)
        run: |
          mkdir -p backup/auth
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          // ‚úÖ FIX: Export system_users table data (custom auth)
          console.log('‚úì Exporting system_users table (custom authentication)...');
          
          let supabaseAuthUsers = [];
          let systemUsers = [];
          
          // First, try to export Supabase Auth users (if any)
          supabase.auth.admin.listUsers()
            .then(({ data, error }) => {
              if (!error && data && data.users) {
                supabaseAuthUsers = data.users;
                console.log('‚úì Found', data.users.length, 'Supabase Auth users');
              } else {
                console.log('‚Ñπ No Supabase Auth users (using custom auth)');
              }
              
              // Now export system_users table
              return supabase
                .from('system_users')
                .select('*')
                .order('created_at', { ascending: false });
            })
            .then(({ data: systemUsersData, error: systemUsersError }) => {
              if (systemUsersError) {
                console.error('‚úó Error fetching system_users:', systemUsersError.message);
                throw systemUsersError;
              }
              
              if (!systemUsersData || systemUsersData.length === 0) {
                console.log('‚Ñπ No users found in system_users table');
                systemUsers = [];
              } else {
                systemUsers = systemUsersData;
                console.log('‚úì Found', systemUsersData.length, 'users in system_users table');
              }
              
              // ‚úÖ FIX: Create users.json with both Supabase Auth users and system_users
              const usersExport = {
                exported_at: new Date().toISOString(),
                supabase_auth_users: supabaseAuthUsers,
                system_users: systemUsers,
                total_users: supabaseAuthUsers.length + systemUsers.length,
                note: systemUsers.length > 0 
                  ? 'Users exported from system_users table (custom authentication system)'
                  : 'No users found in system_users table'
              };
              
              fs.writeFileSync(
                'backup/auth/users.json',
                JSON.stringify(usersExport, null, 2)
              );
              
              console.log('‚úì Exported users to backup/auth/users.json');
              console.log('  - Supabase Auth users:', supabaseAuthUsers.length);
              console.log('  - System users (system_users table):', systemUsers.length);
              console.log('  - Total users:', usersExport.total_users);
              
              process.exit(0);
            })
            .catch((err) => {
              // If Supabase Auth fails, still try to export system_users
              console.log('‚Ñπ Supabase Auth not accessible, exporting system_users only...');
              
              supabase
                .from('system_users')
                .select('*')
                .order('created_at', { ascending: false })
                .then(({ data: systemUsersData, error: systemUsersError }) => {
                  if (systemUsersError) {
                    console.error('‚úó Error fetching system_users:', systemUsersError.message);
                    throw systemUsersError;
                  }
                  
                  const systemUsers = systemUsersData || [];
                  console.log('‚úì Found', systemUsers.length, 'users in system_users table');
                  
                  const usersExport = {
                    exported_at: new Date().toISOString(),
                    supabase_auth_users: [],
                    system_users: systemUsers,
                    total_users: systemUsers.length,
                    note: systemUsers.length > 0 
                      ? 'Users exported from system_users table (custom authentication system). Supabase Auth not accessible.'
                      : 'No users found in system_users table'
                  };
                  
                  fs.writeFileSync(
                    'backup/auth/users.json',
                    JSON.stringify(usersExport, null, 2)
                  );
                  
                  console.log('‚úì Exported', systemUsers.length, 'system users to backup/auth/users.json');
                  process.exit(0);
                })
                .catch((finalErr) => {
                  console.error('‚úó Failed to export users:', finalErr.message);
                  
                  // Create a note file as fallback
                  fs.writeFileSync(
                    'backup/auth/users.json',
                    JSON.stringify({
                      exported_at: new Date().toISOString(),
                      supabase_auth_users: [],
                      system_users: [],
                      total_users: 0,
                      error: finalErr.message,
                      note: 'Failed to export users. Check database connection and permissions.'
                    }, null, 2)
                  );
                  console.log('‚úì Created users.json with error note');
                  process.exit(0); // Don't fail the workflow
                });
            });
          "

      - name: Download Supabase Storage files
        continue-on-error: false  # ‚úÖ Fail if storage backup fails (important data)
        run: |
          mkdir -p backup/storage
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          const path = require('path');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          // ‚úÖ FIX: Auto-detect buckets if SUPABASE_BUCKETS_TO_BACKUP is not set
          let buckets = (process.env.SUPABASE_BUCKETS_TO_BACKUP || '')
            .split(',')
            .map(b => b.trim())
            .filter(Boolean);
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          // ‚úÖ FIX: Auto-detect buckets if not specified
          if (buckets.length === 0) {
            console.log('‚Ñπ SUPABASE_BUCKETS_TO_BACKUP not set, auto-detecting buckets...');
            try {
              const { data: allBuckets, error: listError } = await supabase.storage.listBuckets();
              if (listError) {
                console.error('‚úó Error listing buckets:', listError.message);
                throw listError;
              }
              
              if (allBuckets && allBuckets.length > 0) {
                buckets = allBuckets.map(b => b.id).filter(id => id !== 's3'); // Exclude 's3' bucket name (it's a metadata identifier)
                console.log('‚úì Auto-detected buckets:', buckets.join(', '));
              } else {
                console.log('‚Ñπ No Supabase Storage buckets found');
                // Create a note about S3 storage
                fs.writeFileSync(
                  'backup/storage/note.txt',
                  'No Supabase Storage buckets found.\\n' +
                  'If your files are stored in AWS S3, they are not included in this backup.\\n' +
                  'S3 files should be backed up separately using AWS backup tools or S3 lifecycle policies.\\n' +
                  'File metadata (file_metadata table) is included in the database backup.'
                );
                console.log('‚úì Created storage backup note');
                process.exit(0);
              }
            } catch (detectErr) {
              console.error('‚úó Error auto-detecting buckets:', detectErr.message);
              // Create a note file
              fs.writeFileSync(
                'backup/storage/note.txt',
                'Could not detect storage buckets.\\n' +
                'Error: ' + detectErr.message + '\\n' +
                'Set SUPABASE_BUCKETS_TO_BACKUP secret in GitHub Settings to specify buckets manually.\\n' +
                'Example: profile-pictures,contracts,inventory,employees,branding,payroll,assets,custody'
              );
              process.exit(0); // Don't fail the workflow
            }
          } else {
            console.log('‚úì Using specified buckets:', buckets.join(', '));
          }
          
          async function downloadBucket(bucketName) {
            try {
              console.log('  ‚Üí Listing files in bucket:', bucketName);
              const { data: files, error } = await supabase.storage
                .from(bucketName)
                .list('', { limit: 1000, recursive: true });
              
              if (error) {
                console.error('  ‚úó Error listing files in', bucketName, ':', error.message);
                return { bucket: bucketName, downloaded: 0, error: error.message };
              }
              
              if (!files || files.length === 0) {
                console.log('  ‚Ñπ No files in bucket:', bucketName);
                return { bucket: bucketName, downloaded: 0 };
              }
              
              console.log('  ‚Üí Found', files.length, 'files in', bucketName);
              const bucketDir = path.join('backup/storage', bucketName);
              fs.mkdirSync(bucketDir, { recursive: true });
              
              let downloaded = 0;
              let failed = 0;
              
              for (const file of files) {
                // Skip directories (they don't have an id)
                if (!file.id || file.name.endsWith('/')) {
                  continue;
                }
                
                try {
                  const { data, error: downloadError } = await supabase.storage
                    .from(bucketName)
                    .download(file.name);
                  
                  if (downloadError) {
                    console.error('    ‚úó Error downloading', file.name, ':', downloadError.message);
                    failed++;
                    continue;
                  }
                  
                  const filePath = path.join(bucketDir, file.name);
                  const dir = path.dirname(filePath);
                  fs.mkdirSync(dir, { recursive: true });
                  fs.writeFileSync(filePath, Buffer.from(await data.arrayBuffer()));
                  downloaded++;
                  
                  if (downloaded % 10 === 0) {
                    console.log('    ‚Üí Downloaded', downloaded, 'files...');
                  }
                } catch (fileErr) {
                  console.error('    ‚úó Error processing', file.name, ':', fileErr.message);
                  failed++;
                }
              }
              
              console.log('  ‚úì Downloaded', downloaded, 'files from', bucketName, (failed > 0 ? '(failed: ' + failed + ')' : ''));
              return { bucket: bucketName, downloaded, failed };
            } catch (bucketErr) {
              console.error('  ‚úó Error backing up bucket', bucketName, ':', bucketErr.message);
              return { bucket: bucketName, downloaded: 0, error: bucketErr.message };
            }
          }
          
          Promise.all(buckets.map(downloadBucket))
            .then((results) => {
              const totalDownloaded = results.reduce((sum, r) => sum + (r.downloaded || 0), 0);
              const totalFailed = results.reduce((sum, r) => sum + (r.failed || 0), 0);
              
              console.log('‚úì Storage backup completed');
              console.log('  - Buckets processed:', buckets.length);
              console.log('  - Total files downloaded:', totalDownloaded);
              if (totalFailed > 0) {
                console.log('  ‚ö†Ô∏è Failed downloads:', totalFailed);
              }
              
              // Check if any buckets had errors
              const bucketsWithErrors = results.filter(r => r.error);
              if (bucketsWithErrors.length > 0) {
                console.error('  ‚úó Errors in buckets:', bucketsWithErrors.map(r => r.bucket).join(', '));
                throw new Error(`Failed to backup ${bucketsWithErrors.length} bucket(s): ${bucketsWithErrors.map(r => r.bucket + ' (' + r.error + ')').join(', ')}`);
              }
              
              // Create a summary file
              fs.writeFileSync(
                'backup/storage/backup_summary.json',
                JSON.stringify({
                  exported_at: new Date().toISOString(),
                  buckets_backed_up: buckets,
                  results: results,
                  total_files_downloaded: totalDownloaded,
                  total_failed: totalFailed,
                  note: 'Supabase Storage buckets backed up. S3 files are backed up in a separate step (backup/storage/s3/).'
                }, null, 2)
              );
              
              if (totalDownloaded === 0 && buckets.length > 0) {
                console.log('  ‚ÑπÔ∏è No files found in any buckets (this may be normal if buckets are empty)');
              }
            })
            .catch((err) => {
              console.error('‚úó Failed to download storage files:', err);
              console.error('‚úó Error details:', err.message);
              // Create error note
              fs.writeFileSync(
                'backup/storage/error_note.txt',
                'Storage backup failed: ' + err.message + '\\n' +
                'File metadata is still available in the database backup (file_metadata table).'
              );
              throw err; // Fail the workflow
            });
          "

      - name: Download S3 Storage files
        continue-on-error: true  # ‚úÖ Continue even if S3 backup fails
        run: |
          mkdir -p backup/storage/s3
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const { S3Client, GetObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const path = require('path');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const awsAccessKeyId = process.env.AWS_ACCESS_KEY_ID;
          const awsSecretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;
          const awsRegion = process.env.AWS_S3_REGION || 'us-east-1';
          const awsBucket = process.env.AWS_S3_BUCKET;
          const backupS3Files = process.env.BACKUP_S3_FILES !== 'false'; // Default to true
          
          if (!backupS3Files) {
            console.log('‚Ñπ S3 file backup is disabled (BACKUP_S3_FILES=false)');
            fs.writeFileSync(
              'backup/storage/s3/note.txt',
              'S3 file backup is disabled.\\n' +
              'Set BACKUP_S3_FILES=true in GitHub Secrets to enable S3 file backup.'
            );
            process.exit(0);
          }
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            process.exit(1);
          }
          
          if (!awsAccessKeyId || !awsSecretAccessKey || !awsBucket) {
            console.log('‚Ñπ AWS S3 credentials not configured');
            console.log('‚Ñπ S3 files will not be backed up, but file metadata is in database backup');
            fs.writeFileSync(
              'backup/storage/s3/note.txt',
              'AWS S3 credentials not configured in GitHub Secrets.\\n' +
              'Required secrets:\\n' +
              '  - AWS_ACCESS_KEY_ID\\n' +
              '  - AWS_SECRET_ACCESS_KEY\\n' +
              '  - AWS_S3_BUCKET\\n' +
              '  - AWS_S3_REGION (optional, defaults to us-east-1)\\n' +
              '\\n' +
              'File metadata (paths, names, sizes) is still included in the database backup.'
            );
            process.exit(0);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          // Initialize S3 client
          const s3Client = new S3Client({
            region: awsRegion,
            credentials: {
              accessKeyId: awsAccessKeyId,
              secretAccessKey: awsSecretAccessKey,
            },
          });
          
          console.log('‚úì Fetching S3 file list from file_metadata table...');
          
          // Get all files from file_metadata where bucket = 's3' and not deleted
          let offset = 0;
          const limit = 1000;
          let allFiles = [];
          let hasMore = true;
          
          while (hasMore) {
            const { data: files, error } = await supabase
              .from('file_metadata')
              .select('id, path, file_name, mime_type, size, category, owner_id, owner_type, created_at')
              .eq('bucket', 's3')
              .is('deleted_at', null)
              .range(offset, offset + limit - 1)
              .order('created_at', { ascending: false });
            
            if (error) {
              console.error('‚úó Error fetching file metadata:', error.message);
              throw error;
            }
            
            if (!files || files.length === 0) {
              hasMore = false;
              break;
            }
            
            allFiles = allFiles.concat(files);
            offset += limit;
            hasMore = files.length === limit;
          }
          
          if (allFiles.length === 0) {
            console.log('‚Ñπ No S3 files found in file_metadata table');
            fs.writeFileSync(
              'backup/storage/s3/note.txt',
              'No S3 files found in file_metadata table.\\n' +
              'All files may be stored in Supabase Storage instead of S3.'
            );
            process.exit(0);
          }
          
          console.log('‚úì Found', allFiles.length, 'S3 files to backup');
          console.log('  ‚Üí Starting download from S3 bucket:', awsBucket);
          
          let downloaded = 0;
          let failed = 0;
          const failedFiles = [];
          
          for (const file of allFiles) {
            try {
              const s3Key = file.path; // The path stored in file_metadata is the S3 key
              
              // Download file from S3
              const getObjectCommand = new GetObjectCommand({
                Bucket: awsBucket,
                Key: s3Key,
              });
              
              const response = await s3Client.send(getObjectCommand);
              
              // Read the file stream
              const chunks = [];
              for await (const chunk of response.Body) {
                chunks.push(chunk);
              }
              const fileBuffer = Buffer.concat(chunks);
              
              // Save to backup directory, preserving the path structure
              const backupPath = path.join('backup/storage/s3', s3Key);
              const backupDir = path.dirname(backupPath);
              fs.mkdirSync(backupDir, { recursive: true });
              fs.writeFileSync(backupPath, fileBuffer);
              
              downloaded++;
              
              if (downloaded % 10 === 0) {
                console.log('  ‚Üí Downloaded', downloaded, '/', allFiles.length, 'files...');
              }
            } catch (fileErr) {
              failed++;
              failedFiles.push({
                path: file.path,
                file_name: file.file_name,
                error: fileErr.message
              });
              console.error('  ‚úó Failed to download', file.path, ':', fileErr.message);
            }
          }
          
          console.log('‚úì S3 backup completed');
          console.log('  - Files downloaded:', downloaded, '/', allFiles.length);
          if (failed > 0) {
            console.log('  - Failed downloads:', failed);
          }
          
          // Create summary file
          fs.writeFileSync(
            'backup/storage/s3/backup_summary.json',
            JSON.stringify({
              exported_at: new Date().toISOString(),
              total_files: allFiles.length,
              downloaded: downloaded,
              failed: failed,
              s3_bucket: awsBucket,
              s3_region: awsRegion,
              failed_files: failedFiles,
              note: 'S3 files backed up from file_metadata table. File metadata is also in database backup.'
            }, null, 2)
          );
          
          if (failed > 0) {
            // Create failed files list
            fs.writeFileSync(
              'backup/storage/s3/failed_files.json',
              JSON.stringify(failedFiles, null, 2)
            );
            console.log('  ‚ö† Created failed_files.json with', failed, 'failed downloads');
          }
          "

      - name: Create backup archive
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%d-%H-%M-UTC")
          BACKUP_NAME="backup-$TIMESTAMP.zip"
          zip -r "$BACKUP_NAME" backup/
          echo "BACKUP_FILE=$BACKUP_NAME" >> $GITHUB_ENV
          echo "BACKUP_SIZE=$(stat -f%z "$BACKUP_NAME" 2>/dev/null || stat -c%s "$BACKUP_NAME")" >> $GITHUB_ENV
          echo "‚úì Created backup archive: $BACKUP_NAME"

      - name: Upload to S3
        run: |
          S3_PATH="backups/$(date -u +"%Y/%m/%d")/${{ env.BACKUP_FILE }}"
          aws s3 cp "${{ env.BACKUP_FILE }}" "s3://$AWS_S3_BUCKET/$S3_PATH" \
            --region "$AWS_S3_REGION"
          echo "S3_KEY=$S3_PATH" >> $GITHUB_ENV
          echo "‚úì Uploaded to S3: $S3_PATH"

      - name: Update backup_history and system_settings
        # ‚úÖ FIX: Pass variables via command line, not export
        # Variables from env: are already available, but we pass STATUS/ERROR_TEXT explicitly
        run: |
          # Check if database backup failed (network issue)
          if [ -f backup/db/backup.sql ] && grep -q "DATABASE BACKUP FAILED" backup/db/backup.sql; then
            FINAL_STATUS="partial"
            FINAL_ERROR_TEXT="Database backup failed: Network unreachable (Supabase Free Plan blocks external connections). Auth users and storage files backed up successfully. Enable IP whitelisting or upgrade to Pro plan for complete backup."
          else
            FINAL_STATUS="success"
            FINAL_ERROR_TEXT=""
          fi
          
          S3_KEY="${{ env.S3_KEY }}"
          BACKUP_SIZE="${{ env.BACKUP_SIZE }}"
          
          # ‚úÖ FIX: Pass variables directly to Node.js process
          STATUS="$FINAL_STATUS" \
          ERROR_TEXT="$FINAL_ERROR_TEXT" \
          S3_KEY="$S3_KEY" \
          BACKUP_SIZE="$BACKUP_SIZE" \
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          
          // ‚úÖ FIX: Get all variables from process.env
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const dispatchId = process.env.DISPATCH_ID;
          const status = process.env.STATUS;
          const errorText = process.env.ERROR_TEXT || null;
          const s3Key = process.env.S3_KEY;
          const sizeBytes = process.env.BACKUP_SIZE ? parseInt(process.env.BACKUP_SIZE, 10) : null;
          
          // ‚úÖ FIX: Validate all required variables
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó CRITICAL ERROR: Missing Supabase credentials');
            console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
            console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó CRITICAL ERROR: Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            console.error('');
            console.error('Please check your VITE_SUPABASE_URL secret in GitHub Settings');
            console.error('It should be your Supabase project URL, NOT the database connection string');
            console.error('Correct: https://rqssjgiunwyjeyutgkkp.supabase.co');
            console.error('Wrong:   postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          if (!dispatchId) {
            console.error('‚úó CRITICAL ERROR: Missing DISPATCH_ID');
            process.exit(1);
          }
          
          console.log('‚úì Supabase credentials validated');
          console.log('‚úì URL format validated');
          console.log('‚úì Updating backup_history with:');
          console.log('  - dispatch_id:', dispatchId);
          console.log('  - status:', status);
          console.log('  - s3_key:', s3Key);
          console.log('  - size_bytes:', sizeBytes);
          
          const supabase = createClient(cleanUrl, supabaseKey);
          
          // Update backup_history
          supabase
            .from('backup_history')
            .update({
              status: status,
              s3_key: s3Key,
              size_bytes: sizeBytes,
              error_text: errorText,
            })
            .eq('dispatch_id', dispatchId)
            .then(({ data, error }) => {
              if (error) {
                console.error('‚úó CRITICAL ERROR updating backup history');
                console.error('Error:', error);
                console.error('Error code:', error.code);
                console.error('Error message:', error.message);
                console.error('Error details:', error.details);
                console.error('Error hint:', error.hint);
                process.exit(1);
              }
              
              console.log('‚úì Backup history updated successfully');
              
              // Update system_settings_kv
              return supabase
                .from('system_settings_kv')
                .upsert({
                  key: 'last_backup_at',
                  value: new Date().toISOString(),
                }, {
                  onConflict: 'key',
                });
            })
            .then(({ error: settingsError }) => {
              if (settingsError) {
                console.error('‚ö† Warning: Failed to update last_backup_at:', settingsError.message);
                // Don't fail the step, just log the warning
              } else {
                console.log('‚úì System settings updated');
              }
            })
            .catch((err) => {
              console.error('‚úó CRITICAL ERROR:', err);
              process.exit(1);
            });
          "

      - name: Handle backup failure
        if: failure()
        # ‚úÖ FIX: Use the same secret names as job-level env
        env:
          SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
        run: |
          FINAL_STATUS="failed"
          FINAL_ERROR_TEXT="Backup workflow failed. Check GitHub Actions logs for details."
          
          # Debug: Check if variables are available
          echo "Debug: Checking environment variables in failure handler..."
          echo "SUPABASE_URL: ${SUPABASE_URL:+SET (hidden)}${SUPABASE_URL:-MISSING}"
          echo "SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY:+SET (hidden)}${SUPABASE_SERVICE_ROLE_KEY:-MISSING}"
          echo "DISPATCH_ID: $DISPATCH_ID"
          
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] || [ -z "$DISPATCH_ID" ]; then
            echo "‚ùå Cannot update backup status - missing required variables"
            echo "This is a critical error. Please check GitHub Secrets configuration."
            exit 1
          fi
          
          STATUS="$FINAL_STATUS" \
          ERROR_TEXT="$FINAL_ERROR_TEXT" \
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const dispatchId = process.env.DISPATCH_ID;
          const status = process.env.STATUS;
          const errorText = process.env.ERROR_TEXT;
          
          if (!supabaseUrl || !supabaseKey || !dispatchId) {
            console.error('‚úó Cannot update backup status - missing credentials');
            console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
            console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
            console.error('DISPATCH_ID:', dispatchId ? 'SET' : 'MISSING');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Remove trailing slash if present
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          console.log('‚úì Updating backup status to failed');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('backup_history')
            .update({
              status: status,
              error_text: errorText,
            })
            .eq('dispatch_id', dispatchId)
            .then(({ error }) => {
              if (error) {
                console.error('‚úó Failed to update backup status to failed:', error.message);
                process.exit(1);
              } else {
                console.log('‚úì Backup status updated to failed');
              }
            })
            .catch((err) => {
              console.error('‚úó Error updating backup status:', err);
              process.exit(1);
            });
          "
