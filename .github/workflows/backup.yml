name: Database and Storage Backup

on:
  schedule:
    # Run daily at 2:00 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      dispatch_id:
        description: 'Dispatch ID for manual backups'
        required: false
        type: string
      trigger_type:
        description: 'Type of trigger (manual or scheduled)'
        required: false
        type: string
        default: 'scheduled'

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
  SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_S3_REGION: ${{ secrets.AWS_S3_REGION }}
  AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
  SUPABASE_BUCKETS_TO_BACKUP: ${{ secrets.SUPABASE_BUCKETS_TO_BACKUP }}

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          npm install @supabase/supabase-js

      - name: Configure AWS CLI
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region ${{ secrets.AWS_S3_REGION }}

      - name: Check backup enabled status
        id: check_backup_enabled
        run: |
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_SERVICE_ROLE_KEY);
          
          const triggerType = process.env.TRIGGER_TYPE || 'scheduled';
          console.log('Trigger type:', triggerType);
          
          supabase
            .from('system_settings_kv')
            .select('value')
            .eq('key', 'backup_enabled')
            .single()
            .then(({ data, error }) => {
              if (error) {
                console.error('Error checking backup status:', error);
                // For manual triggers, continue even if check fails
                if (triggerType === 'manual') {
                  console.log('Manual trigger - continuing despite check error');
                  process.exit(0);
                }
                process.exit(1);
              }
              const enabled = data?.value?.enabled ?? false;
              console.log('Backup enabled:', enabled);
              // For manual triggers, always run regardless of enabled status
              if (!enabled && triggerType !== 'manual') {
                console.log('Backup is disabled and this is a scheduled run. Exiting.');
                process.exit(0);
              }
              if (triggerType === 'manual') {
                console.log('Manual trigger - proceeding with backup');
              }
              process.exit(0);
            });
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          TRIGGER_TYPE: ${{ github.event.inputs.trigger_type || 'scheduled' }}
          
      - name: Log workflow inputs
        run: |
          echo "=========================================="
          echo "Workflow Inputs & Configuration"
          echo "=========================================="
          echo "Dispatch ID (input): ${{ github.event.inputs.dispatch_id }}"
          echo "Dispatch ID (fallback): ${{ github.run_id }}"
          echo "Final Dispatch ID: ${{ github.event.inputs.dispatch_id || github.run_id }}"
          echo "Trigger Type: ${{ github.event.inputs.trigger_type || 'scheduled' }}"
          echo "Workflow Run ID: ${{ github.run_id }}"
          echo "Branch: ${{ github.ref }}"
          echo "=========================================="

      - name: Create backup directory
        run: |
          mkdir -p backup/{db,storage,auth}

      - name: Dump PostgreSQL database
        id: pg_dump
        timeout-minutes: 30
        run: |
          echo "=========================================="
          echo "Starting database dump..."
          echo "=========================================="
          echo "Using compressed custom format for faster processing..."
          
          # Use custom format with compression for faster processing
          # Using --dbname flag with direct PostgreSQL connection URL (port 5432)
          pg_dump --dbname="$SUPABASE_DB_URL" \
            --no-owner \
            --no-privileges \
            --format=custom \
            --compress=6 \
            --file=backup/db/backup.dump \
            --verbose 2>&1 | tee backup/db/dump.log || {
            echo "ERROR: Database dump failed"
            cat backup/db/dump.log
            exit 1
          }
          
          # Convert to SQL format for compatibility (optional, but slower)
          # For restore, users can use pg_restore with the .dump file
          echo "Converting to SQL format for compatibility..."
          pg_restore --dbname="$SUPABASE_DB_URL" \
            --no-owner \
            --no-privileges \
            --format=plain \
            backup/db/backup.dump \
            > backup/db/backup.sql 2>&1 || {
            echo "WARNING: SQL conversion failed, but .dump file is available"
            # Create empty SQL file so archive doesn't fail
            touch backup/db/backup.sql
          }
          
          DB_SIZE=$(du -b backup/db/backup.dump | cut -f1)
          SQL_SIZE=$(du -b backup/db/backup.sql | cut -f1)
          echo "Database dump size: $DB_SIZE bytes (.dump format)"
          echo "SQL dump size: $SQL_SIZE bytes (.sql format)"
          echo "db_size=$DB_SIZE" >> $GITHUB_OUTPUT
          echo "sql_size=$SQL_SIZE" >> $GITHUB_OUTPUT

      - name: Export auth users
        id: export_auth
        run: |
          echo "Exporting auth users..."
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          
          const supabase = createClient(
            process.env.SUPABASE_URL,
            process.env.SUPABASE_SERVICE_ROLE_KEY
          );
          
          // Query auth.users via service role
          supabase.auth.admin.listUsers()
            .then(({ data, error }) => {
              if (error) {
                console.error('Error fetching auth users:', error);
                process.exit(1);
              }
              
              const users = data.users.map(u => ({
                id: u.id,
                email: u.email,
                created_at: u.created_at,
                updated_at: u.updated_at,
                email_confirmed_at: u.email_confirmed_at,
                last_sign_in_at: u.last_sign_in_at,
                user_metadata: u.user_metadata,
                app_metadata: u.app_metadata
              }));
              
              fs.writeFileSync(
                'backup/auth/users.json',
                JSON.stringify(users, null, 2)
              );
              
              const size = fs.statSync('backup/auth/users.json').size;
              console.log('Auth users exported:', users.length);
              console.log('Auth file size:', size, 'bytes');
              process.exit(0);
            });
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: Update backup status (in progress)
        id: update_status_progress
        run: |
          node -e "
          (async () => {
            try {
              const { createClient } = require('@supabase/supabase-js');
              const supabase = createClient(
                process.env.SUPABASE_URL,
                process.env.SUPABASE_SERVICE_ROLE_KEY
              );
              const dispatchId = process.env.DISPATCH_ID || '';
              
              // Update status to show we're processing storage
              const { data: existingEntry } = await supabase
                .from('backup_history')
                .select('id')
                .eq('dispatch_id', dispatchId)
                .maybeSingle();
              
              if (existingEntry) {
                await supabase
                  .from('backup_history')
                  .update({ 
                    status: 'in_progress',
                    error_text: 'Processing database and storage files...'
                  })
                  .eq('id', existingEntry.id);
              }
              process.exit(0);
            } catch (err) {
              console.warn('Failed to update progress status:', err.message);
              process.exit(0); // Don't fail the workflow
            }
          })();
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}

      - name: Download Supabase Storage files
        id: download_storage
        timeout-minutes: 20
        run: |
          echo "=========================================="
          echo "Downloading storage files..."
          echo "=========================================="
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          const path = require('path');
          
          const supabase = createClient(
            process.env.SUPABASE_URL,
            process.env.SUPABASE_SERVICE_ROLE_KEY
          );
          
          const buckets = (process.env.SUPABASE_BUCKETS_TO_BACKUP || '')
            .split(',')
            .map(b => b.trim())
            .filter(b => b.length > 0);
          
          if (buckets.length === 0) {
            console.log('No buckets specified for backup - skipping storage backup');
            // Create empty storage directory
            if (!fs.existsSync('backup/storage')) {
              fs.mkdirSync('backup/storage', { recursive: true });
            }
            process.exit(0);
          }
          
          console.log(\`Backing up \${buckets.length} bucket(s): \${buckets.join(', ')}\`);
          
          async function downloadFile(bucket, filePath, localPath) {
            try {
              const { data, error } = await supabase.storage
                .from(bucket)
                .download(filePath);
              
              if (error) {
                console.error(\`Error downloading \${bucket}/\${filePath}:\`, error.message);
                return false;
              }
              
              const dir = path.dirname(localPath);
              if (!fs.existsSync(dir)) {
                fs.mkdirSync(dir, { recursive: true });
              }
              
              const arrayBuffer = await data.arrayBuffer();
              const buffer = Buffer.from(arrayBuffer);
              fs.writeFileSync(localPath, buffer);
              return true;
            } catch (err) {
              console.error(\`Exception downloading \${bucket}/\${filePath}:\`, err.message);
              return false;
            }
          }
          
          async function listFiles(bucket) {
            const files = [];
            
            async function listRecursive(folder = '') {
              try {
                const { data: items, error } = await supabase.storage
                  .from(bucket)
                  .list(folder, { limit: 1000, offset: 0 });
                
                if (error) {
                  console.error(\`Error listing \${bucket}/\${folder}:\`, error.message);
                  return;
                }
                
                for (const item of items || []) {
                  const fullPath = folder ? \`\${folder}/\${item.name}\` : item.name;
                  
                  if (item.id === null) {
                    // It's a folder, recurse
                    await listRecursive(fullPath);
                  } else {
                    // It's a file
                    files.push(fullPath);
                  }
                }
              } catch (err) {
                console.error(\`Exception listing \${bucket}/\${folder}:\`, err.message);
              }
            }
            
            await listRecursive();
            return files;
          }
          
            (async () => {
            const startTime = Date.now();
            let totalFiles = 0;
            let totalSize = 0;
            const MAX_CONCURRENT_DOWNLOADS = 5; // Reduced to 5 for stability
            const MAX_STORAGE_SIZE_MB = 500; // Limit storage backup to 500MB to prevent timeout
            const MAX_STORAGE_FILES = 1000; // Limit to 1000 files to prevent timeout
            
            for (const bucket of buckets) {
              console.log(\`Processing bucket: \${bucket}\`);
              const bucketStartTime = Date.now();
              const files = await listFiles(bucket);
              console.log(\`Found \${files.length} files in \${bucket}\`);
              
              // Limit files if too many to prevent timeout
              let filesToProcess = files;
              if (files.length > MAX_STORAGE_FILES) {
                console.log(\`WARNING: Too many files (\${files.length}). Limiting to first \${MAX_STORAGE_FILES} files to prevent timeout.\`);
                filesToProcess = files.slice(0, MAX_STORAGE_FILES);
              }
              
              // Download files in parallel batches
              for (let i = 0; i < filesToProcess.length; i += MAX_CONCURRENT_DOWNLOADS) {
                const batch = filesToProcess.slice(i, i + MAX_CONCURRENT_DOWNLOADS);
                
                // Check size limit
                if (totalSize > MAX_STORAGE_SIZE_MB * 1024 * 1024) {
                  console.log(\`WARNING: Storage backup size limit reached (\${MAX_STORAGE_SIZE_MB}MB). Stopping download to prevent timeout.\`);
                  break;
                }
                const downloadPromises = batch.map(async (filePath) => {
                  const localPath = \`backup/storage/\${bucket}/\${filePath}\`;
                  const success = await downloadFile(bucket, filePath, localPath);
                  
                  if (success) {
                    const stats = fs.statSync(localPath);
                    return { success: true, size: stats.size };
                  }
                  return { success: false, size: 0 };
                });
                
                const results = await Promise.all(downloadPromises);
                results.forEach(result => {
                  if (result.success) {
                    totalFiles++;
                    totalSize += result.size;
                  }
                });
                
                // Log progress every 10 files
                const processed = Math.min(i + batch.length, filesToProcess.length);
                if (processed % 10 === 0 || processed === filesToProcess.length) {
                  const sizeMB = (totalSize / 1024 / 1024).toFixed(2);
                  console.log(\`Progress: \${processed}/\${filesToProcess.length} files processed (\${sizeMB} MB)\`);
                }
              }
              
              const bucketTime = ((Date.now() - bucketStartTime) / 1000).toFixed(2);
              const bucketSizeMB = (totalSize / 1024 / 1024).toFixed(2);
              console.log(\`Bucket \${bucket} completed in \${bucketTime}s: \${totalFiles} files, \${bucketSizeMB} MB\`);
              
              if (files.length > MAX_STORAGE_FILES) {
                console.log(\`NOTE: Only backed up \${filesToProcess.length} of \${files.length} files due to size limits\`);
              }
            }
            
            const totalTime = ((Date.now() - startTime) / 1000).toFixed(2);
            const totalSizeMB = (totalSize / 1024 / 1024).toFixed(2);
            console.log(\`==========================================\`);
            console.log(\`Storage backup completed in \${totalTime}s\`);
            console.log(\`Total files: \${totalFiles}\`);
            console.log(\`Total size: \${totalSizeMB} MB\`);
            console.log(\`==========================================\`);
          })().catch(err => {
            console.error('Storage download error:', err);
            process.exit(1);
          });
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SUPABASE_BUCKETS_TO_BACKUP: ${{ secrets.SUPABASE_BUCKETS_TO_BACKUP }}

      - name: Create backup archive
        id: create_archive
        timeout-minutes: 10
        run: |
          echo "=========================================="
          echo "Creating backup archive..."
          echo "=========================================="
          TIMESTAMP=$(date -u +"%Y-%m-%d-%H-%M-UTC")
          ARCHIVE_NAME="backup-$TIMESTAMP.zip"
          
          cd backup
          echo "Compressing files..."
          zip -r "../$ARCHIVE_NAME" . -q -9
          cd ..
          
          ARCHIVE_SIZE=$(du -b "$ARCHIVE_NAME" | cut -f1)
          ARCHIVE_SIZE_MB=$(awk "BEGIN {printf \"%.2f\", $ARCHIVE_SIZE / 1024 / 1024}")
          echo "Archive created: $ARCHIVE_NAME"
          echo "Archive size: $ARCHIVE_SIZE bytes ($ARCHIVE_SIZE_MB MB)"
          echo "archive_name=$ARCHIVE_NAME" >> $GITHUB_OUTPUT
          echo "archive_size=$ARCHIVE_SIZE" >> $GITHUB_OUTPUT

      - name: Upload to S3 (with retry)
        id: upload_s3
        timeout-minutes: 15
        run: |
          ARCHIVE_NAME="${{ steps.create_archive.outputs.archive_name }}"
          TIMESTAMP=$(date -u +"%Y-%m-%d")
          S3_KEY="backups/$TIMESTAMP/$ARCHIVE_NAME"
          S3_BUCKET="${{ secrets.AWS_S3_BUCKET }}"
          S3_REGION="${{ secrets.AWS_S3_REGION }}"
          
          echo "=========================================="
          echo "Uploading backup to S3..."
          echo "=========================================="
          echo "Archive: $ARCHIVE_NAME"
          echo "S3 Bucket: $S3_BUCKET"
          echo "S3 Region: $S3_REGION"
          echo "S3 Key: $S3_KEY"
          echo "Full S3 Path: s3://$S3_BUCKET/$S3_KEY"
          
          if [ -z "$S3_BUCKET" ] || [ -z "$S3_REGION" ]; then
            echo "ERROR: AWS S3 configuration is missing!"
            echo "S3_BUCKET: ${S3_BUCKET:-NOT SET}"
            echo "S3_REGION: ${S3_REGION:-NOT SET}"
            exit 1
          fi
          
          if [ ! -f "$ARCHIVE_NAME" ]; then
            echo "ERROR: Archive file not found: $ARCHIVE_NAME"
            exit 1
          fi
          
          ARCHIVE_SIZE=$(du -h "$ARCHIVE_NAME" | cut -f1)
          echo "Archive size: $ARCHIVE_SIZE"
          
          MAX_RETRIES=3
          RETRY_COUNT=0
          UPLOAD_SUCCESS=false
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ] && [ "$UPLOAD_SUCCESS" = false ]; do
            echo ""
            echo "Upload attempt $((RETRY_COUNT + 1)) of $MAX_RETRIES..."
            
            if aws s3 cp "$ARCHIVE_NAME" "s3://$S3_BUCKET/$S3_KEY" --region "$S3_REGION"; then
              echo "✓ Upload completed!"
              UPLOAD_SUCCESS=true
              
              # Verify the upload
              echo "Verifying upload..."
              if aws s3 ls "s3://$S3_BUCKET/$S3_KEY" --region "$S3_REGION" > /dev/null 2>&1; then
                echo "✓ Upload verified successfully"
                echo "s3_key=$S3_KEY" >> $GITHUB_OUTPUT
                echo ""
                echo "Backup successfully stored at:"
                echo "  s3://$S3_BUCKET/$S3_KEY"
              else
                echo "✗ Upload verification failed"
                UPLOAD_SUCCESS=false
              fi
            else
              echo "✗ Upload failed, retrying..."
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                sleep $((RETRY_COUNT * 5))
              fi
            fi
          done
          
          if [ "$UPLOAD_SUCCESS" = false ]; then
            echo ""
            echo "ERROR: S3 upload failed after $MAX_RETRIES attempts"
            echo "Please check:"
            echo "  1. AWS credentials are correct"
            echo "  2. S3 bucket exists: $S3_BUCKET"
            echo "  3. IAM permissions allow upload to bucket"
            exit 1
          fi

      - name: Update backup_history and system_settings
        id: update_history
        if: always()  # Always run, even on failure
        run: |
          ARCHIVE_SIZE="${{ steps.create_archive.outputs.archive_size }}"
          S3_KEY="${{ steps.upload_s3.outputs.s3_key }}"
          DISPATCH_ID="${{ github.event.inputs.dispatch_id || github.run_id }}"
          WORKFLOW_RUN_ID="${{ github.run_id }}"
          
          # Check if previous steps succeeded
          UPLOAD_OUTCOME="${{ steps.upload_s3.outcome }}"
          if [ "$UPLOAD_OUTCOME" = "success" ] && [ -n "$S3_KEY" ]; then
            FINAL_STATUS="success"
            FINAL_ERROR_TEXT=""
            echo "=========================================="
            echo "Updating backup history (SUCCESS)"
            echo "=========================================="
          else
            FINAL_STATUS="failed"
            FINAL_ERROR_TEXT="Backup workflow completed but S3 upload may have failed. Check GitHub Actions logs for details."
            echo "=========================================="
            echo "Updating backup history (FAILURE/PARTIAL)"
            echo "=========================================="
          fi
          
          echo "Dispatch ID: $DISPATCH_ID"
          echo "Workflow Run ID: $WORKFLOW_RUN_ID"
          echo "S3 Key: ${S3_KEY:-NOT UPLOADED}"
          echo "Archive Size: ${ARCHIVE_SIZE:-0} bytes"
          echo "Status: $FINAL_STATUS"
          echo ""
          
          # Use IIFE to properly handle async/await
          # Pass STATUS and ERROR_TEXT as environment variables to Node.js
          STATUS="$FINAL_STATUS" ERROR_TEXT="$FINAL_ERROR_TEXT" node -e "
          (async () => {
            try {
              const { createClient } = require('@supabase/supabase-js');
              
              const supabase = createClient(
                process.env.SUPABASE_URL,
                process.env.SUPABASE_SERVICE_ROLE_KEY
              );
              
              const now = new Date().toISOString();
              const dispatchId = process.env.DISPATCH_ID || '';
              const status = process.env.STATUS || 'failed';
              const errorText = process.env.ERROR_TEXT || null;
              
              console.log('[UPDATE] Looking for backup entry with dispatch_id:', dispatchId);
              console.log('[UPDATE] Status to set:', status);
              if (errorText) {
                console.log('[UPDATE] Error text:', errorText);
              }
              
              // Find existing entry by dispatch_id
              const { data: existingEntry, error: findError } = await supabase
                .from('backup_history')
                .select('id, dispatch_id, status')
                .eq('dispatch_id', dispatchId)
                .maybeSingle();
              
              if (findError) {
                console.error('[UPDATE] Error finding entry:', JSON.stringify(findError, null, 2));
                throw new Error('Failed to find backup entry: ' + findError.message);
              }
              
              console.log('[UPDATE] Existing entry:', existingEntry ? 
                'Found (ID: ' + existingEntry.id + ', Status: ' + existingEntry.status + ')' : 
                'Not found - will create new');
              
              let updateResult;
              if (existingEntry) {
                // Update existing entry
                console.log('[UPDATE] Updating existing entry...');
                updateResult = await supabase
                  .from('backup_history')
                  .update({
                    workflow_run_id: process.env.WORKFLOW_RUN_ID,
                    s3_key: process.env.S3_KEY || null,
                    status: process.env.STATUS || 'failed',
                    size_bytes: parseInt(process.env.ARCHIVE_SIZE || '0'),
                    error_text: process.env.ERROR_TEXT || null,
                  })
                  .eq('id', existingEntry.id)
                  .select();
                
                if (updateResult.error) {
                  console.error('[UPDATE] Update error:', JSON.stringify(updateResult.error, null, 2));
                  throw new Error('Failed to update backup entry: ' + updateResult.error.message);
                }
                
                console.log('[UPDATE] ✓ Backup entry updated successfully');
                console.log('[UPDATE] Updated entry:', JSON.stringify(updateResult.data, null, 2));
              } else {
                // Insert new entry
                console.log('[UPDATE] Creating new backup entry...');
                updateResult = await supabase
                  .from('backup_history')
                  .insert({
                    dispatch_id: dispatchId || null,
                    workflow_run_id: process.env.WORKFLOW_RUN_ID,
                    s3_key: process.env.S3_KEY || null,
                    status: process.env.STATUS || 'failed',
                    size_bytes: parseInt(process.env.ARCHIVE_SIZE || '0'),
                    error_text: process.env.ERROR_TEXT || null,
                    created_at: now
                  })
                  .select();
                
                if (updateResult.error) {
                  console.error('[UPDATE] Insert error:', JSON.stringify(updateResult.error, null, 2));
                  throw new Error('Failed to insert backup entry: ' + updateResult.error.message);
                }
                
                console.log('[UPDATE] ✓ Backup entry created successfully');
                console.log('[UPDATE] Created entry:', JSON.stringify(updateResult.data, null, 2));
              }
              
              // Update last_backup_at
              console.log('[UPDATE] Updating last_backup_at setting...');
              const { error: settingsError, data: settingsData } = await supabase
                .from('system_settings_kv')
                .upsert({
                  key: 'last_backup_at',
                  value: { timestamp: now }
                }, {
                  onConflict: 'key'
                })
                .select();
              
              if (settingsError) {
                console.error('[UPDATE] Settings update error:', JSON.stringify(settingsError, null, 2));
                // Don't fail the whole step if settings update fails
                console.warn('[UPDATE] Warning: Failed to update last_backup_at, but backup status was updated');
              } else {
                console.log('[UPDATE] ✓ last_backup_at updated successfully');
              }
              
              console.log('');
              console.log('==========================================');
              console.log('✓ Backup history updated successfully');
              console.log('✓ Backup stored at S3 key:', process.env.S3_KEY);
              console.log('==========================================');
              
              process.exit(0);
            } catch (error) {
              console.error('');
              console.error('==========================================');
              console.error('✗ CRITICAL ERROR updating backup history');
              console.error('==========================================');
              console.error('Error:', error.message);
              console.error('Stack:', error.stack);
              console.error('');
              console.error('This error means the backup completed but status was not updated.');
              console.error('Check Supabase connection, RLS policies, and service role key.');
              process.exit(1);
            }
          })();
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
          WORKFLOW_RUN_ID: ${{ github.run_id }}
          S3_KEY: ${{ steps.upload_s3.outputs.s3_key }}
          ARCHIVE_SIZE: ${{ steps.create_archive.outputs.archive_size }}

      - name: Handle backup failure
        if: failure()
        run: |
          DISPATCH_ID="${{ github.event.inputs.dispatch_id || github.run_id }}"
          WORKFLOW_RUN_ID="${{ github.run_id }}"
          ERROR_MSG="Backup workflow failed. Check GitHub Actions logs for details."
          
          echo "=========================================="
          echo "Updating backup history (FAILURE)"
          echo "=========================================="
          echo "Dispatch ID: $DISPATCH_ID"
          echo "Workflow Run ID: $WORKFLOW_RUN_ID"
          echo "Error: $ERROR_MSG"
          echo ""
          
          # Use IIFE to properly handle async/await
          node -e "
          (async () => {
            try {
              const { createClient } = require('@supabase/supabase-js');
              
              const supabase = createClient(
                process.env.SUPABASE_URL,
                process.env.SUPABASE_SERVICE_ROLE_KEY
              );
              
              const dispatchId = process.env.DISPATCH_ID || '';
              
              console.log('[FAILURE] Looking for backup entry with dispatch_id:', dispatchId);
              
              // Find existing entry by dispatch_id
              const { data: existingEntry, error: findError } = await supabase
                .from('backup_history')
                .select('id, dispatch_id, status')
                .eq('dispatch_id', dispatchId)
                .maybeSingle();
              
              if (findError) {
                console.error('[FAILURE] Error finding entry:', JSON.stringify(findError, null, 2));
                // Continue anyway - try to insert new entry
              }
              
              console.log('[FAILURE] Existing entry:', existingEntry ? 
                'Found (ID: ' + existingEntry.id + ', Status: ' + existingEntry.status + ')' : 
                'Not found - will create new');
              
              let updateResult;
              if (existingEntry) {
                // Update existing entry
                console.log('[FAILURE] Updating existing entry to failed status...');
                updateResult = await supabase
                  .from('backup_history')
                  .update({
                    workflow_run_id: process.env.WORKFLOW_RUN_ID,
                    status: 'failed',
                    error_text: process.env.ERROR_MSG,
                    size_bytes: null,
                    s3_key: null
                  })
                  .eq('id', existingEntry.id)
                  .select();
                
                if (updateResult.error) {
                  console.error('[FAILURE] Update error:', JSON.stringify(updateResult.error, null, 2));
                  throw new Error('Failed to update backup entry: ' + updateResult.error.message);
                }
                
                console.log('[FAILURE] ✓ Backup failure recorded (updated)');
                console.log('[FAILURE] Updated entry:', JSON.stringify(updateResult.data, null, 2));
              } else {
                // Insert new entry
                console.log('[FAILURE] Creating new failed backup entry...');
                updateResult = await supabase
                  .from('backup_history')
                  .insert({
                    dispatch_id: dispatchId || null,
                    workflow_run_id: process.env.WORKFLOW_RUN_ID,
                    status: 'failed',
                    error_text: process.env.ERROR_MSG,
                    size_bytes: null,
                    s3_key: null
                  })
                  .select();
                
                if (updateResult.error) {
                  console.error('[FAILURE] Insert error:', JSON.stringify(updateResult.error, null, 2));
                  throw new Error('Failed to insert backup entry: ' + updateResult.error.message);
                }
                
                console.log('[FAILURE] ✓ Backup failure recorded (created)');
                console.log('[FAILURE] Created entry:', JSON.stringify(updateResult.data, null, 2));
              }
              
              console.log('');
              console.log('==========================================');
              console.log('✓ Backup failure status updated');
              console.log('==========================================');
              
              process.exit(0);
            } catch (error) {
              console.error('');
              console.error('==========================================');
              console.error('✗ CRITICAL ERROR updating failure status');
              console.error('==========================================');
              console.error('Error:', error.message);
              console.error('Stack:', error.stack);
              console.error('');
              console.error('The backup failed AND we could not update the status.');
              console.error('Check Supabase connection, RLS policies, and service role key.');
              // Don't exit with error code - we've already failed
              process.exit(0);
            }
          })();
          "
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
          WORKFLOW_RUN_ID: ${{ github.run_id }}
          ERROR_MSG: "Backup workflow failed. Check GitHub Actions logs for details."

      - name: Upload backup as GitHub artifact (optional)
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: backup-${{ steps.create_archive.outputs.archive_name }}
          path: ${{ steps.create_archive.outputs.archive_name }}
          retention-days: 7

      - name: Cleanup
        if: always()
        run: |
          rm -rf backup
          rm -f backup-*.zip

