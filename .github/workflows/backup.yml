name: Backup Database and Storage

on:
  workflow_dispatch:  # Allow manual triggers
    inputs:
      dispatch_id:
        description: 'Unique dispatch ID for tracking this backup'
        required: false
        type: string
      trigger_type:
        description: 'Type of trigger (manual or scheduled)'
        required: false
        type: string
        default: 'manual'
  schedule:
    - cron: '0 2 * * *'  # Daily at 2:00 AM UTC

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    # ‚úÖ FIX: Define all environment variables at job level
    # This makes them available to ALL steps
    # Note: Using VITE_ prefixed secrets to match current GitHub Secrets configuration
    env:
      SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
      SUPABASE_DB_URL: ${{ secrets.DATABASE_URL }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_S3_REGION: ${{ secrets.AWS_S3_REGION }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SUPABASE_BUCKETS_TO_BACKUP: ${{ secrets.SUPABASE_BUCKETS_TO_BACKUP }}
      DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
      TRIGGER_TYPE: ${{ github.event.inputs.trigger_type || 'scheduled' }}
      BACKUP_S3_FILES: ${{ secrets.BACKUP_S3_FILES || 'true' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        # ‚úÖ FIX: Disable submodule checkout to avoid the warning
        with:
          submodules: false

      - name: Debug environment variables
        # ‚úÖ DEBUG: Check if secrets are available (without exposing values)
        run: |
          echo "Checking environment variables..."
          echo "SUPABASE_URL: ${SUPABASE_URL:+SET (hidden)}${SUPABASE_URL:-MISSING}"
          echo "SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY:+SET (hidden)}${SUPABASE_SERVICE_ROLE_KEY:-MISSING}"
          echo "SUPABASE_DB_URL: ${SUPABASE_DB_URL:+SET (hidden)}${SUPABASE_DB_URL:-MISSING}"
          echo "AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:+SET (hidden)}${AWS_ACCESS_KEY_ID:-MISSING}"
          echo "AWS_S3_BUCKET: ${AWS_S3_BUCKET:+SET (hidden)}${AWS_S3_BUCKET:-MISSING}"
          echo "DISPATCH_ID: $DISPATCH_ID"
          
          # Check if secrets are actually set
          if [ -z "$SUPABASE_URL" ]; then
            echo "‚ùå ERROR: VITE_SUPABASE_URL secret is not set in GitHub Secrets"
            echo "Please go to: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo "And add VITE_SUPABASE_URL secret"
            exit 1
          fi
          
          if [ -z "$SUPABASE_SERVICE_ROLE_KEY" ]; then
            echo "‚ùå ERROR: VITE_SUPABASE_SERVICE_ROLE_KEY secret is not set in GitHub Secrets"
            echo "Please go to: Repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions"
            echo "And add VITE_SUPABASE_SERVICE_ROLE_KEY secret"
            exit 1
          fi
          
          echo "‚úì All required secrets are set"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Check backup enable status
        # ‚úÖ FIX: Environment variables are inherited from job-level env:
        run: |
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          
          // ‚úÖ FIX: Get from process.env (automatically available from job env:)
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          // ‚úÖ DEBUG: Validate URL format (without exposing full value)
          console.log('Validating Supabase URL...');
          console.log('URL length:', supabaseUrl ? supabaseUrl.length : 0);
          console.log('URL starts with https://', supabaseUrl ? supabaseUrl.startsWith('https://') : false);
          console.log('URL preview:', supabaseUrl ? supabaseUrl.substring(0, 30) + '...' : 'MISSING');
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
            console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value starts with:', supabaseUrl.substring(0, 20));
            console.error('Expected format: https://xxxxx.supabase.co');
            console.error('');
            console.error('Please check your VITE_SUPABASE_URL secret in GitHub Settings');
            console.error('It should be your Supabase project URL, not the database connection string');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Remove trailing slash if present
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          console.log('‚úì Supabase credentials found');
          console.log('‚úì URL format validated');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('system_settings_kv')
            .select('value')
            .eq('key', 'backup_enabled')
            .single()
            .then(({ data, error }) => {
              if (error && error.code !== 'PGRST116') {
                console.error('‚úó Error checking backup status:', error);
                process.exit(1);
              }
              
              const enabled = data?.value?.enabled ?? false;
              if (!enabled) {
                console.log('‚Ñπ Backup is disabled. Exiting.');
                process.exit(0);
              }
              
              console.log('‚úì Backup is enabled. Proceeding...');
            })
            .catch((err) => {
              console.error('‚úó Failed to check backup status:', err);
              process.exit(1);
            });
          "

      - name: Update backup status (in progress)
        run: |
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const dispatchId = process.env.DISPATCH_ID;
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          if (!dispatchId) {
            console.error('‚úó Missing DISPATCH_ID');
            process.exit(1);
          }
          
          console.log('‚úì Updating backup status to in_progress');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('backup_history')
            .update({ status: 'in_progress' })
            .eq('dispatch_id', dispatchId)
            .then(({ error }) => {
              if (error) {
                console.error('‚úó Error updating backup status:', error);
                process.exit(1);
              }
              console.log('‚úì Backup status updated to in_progress');
            })
            .catch((err) => {
              console.error('‚úó Failed to update backup status:', err);
              process.exit(1);
            });
          "

      - name: Export database
        continue-on-error: false  # ‚úÖ Fail if database export fails (critical step)
        run: |
          mkdir -p backup/db
          
          # ‚úÖ FIX: Verify connection string is set
          if [ -z "$SUPABASE_DB_URL" ]; then
            echo "‚ùå ERROR: DATABASE_URL secret is not set"
            echo "Please set DATABASE_URL secret in GitHub Settings"
            echo "Format: postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres"
            exit 1
          fi
          
          echo "‚úì Starting database export..."
          echo "Connection string format: ${SUPABASE_DB_URL%%@*}@***" # Show only user@host part
          
          # ‚úÖ FIX: Try pg_dump, handle network errors gracefully
          pg_dump "$SUPABASE_DB_URL" \
            --no-owner \
            --no-privileges \
            --format=plain \
            --blobs \
            --verbose \
            --file=backup/db/backup.sql \
            2>&1 | tee /tmp/pg_dump_output.log || {
              PG_DUMP_ERROR=$(cat /tmp/pg_dump_output.log)
              
              # Check if it's a network/unreachable error (Supabase free plan issue)
              if echo "$PG_DUMP_ERROR" | grep -q "Network is unreachable\|Connection refused\|timeout\|unreachable"; then
                echo ""
                echo "‚ö†Ô∏è WARNING: Direct database connection failed (Network unreachable)"
                echo "This is common with Supabase Free Plan - external connections are blocked"
                echo ""
                echo "üîÑ Attempting backup via Supabase Edge Function (bypasses IP restrictions)..."
                echo ""
                
                # ‚úÖ FIX: Use Edge Function to export database via API
                # Write script to file using printf to avoid heredoc issues
                printf '%s\n' \
                  "const { createClient } = require('@supabase/supabase-js');" \
                  "const fs = require('fs');" \
                  "" \
                  "(async () => {" \
                  "  try {" \
                  "    const supabaseUrl = process.env.SUPABASE_URL;" \
                  "    const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;" \
                  "" \
                  "    if (!supabaseUrl || !supabaseKey) {" \
                  "      console.error('‚úó Cannot use Edge Function export - missing credentials');" \
                  "      process.exit(1);" \
                  "    }" \
                  "" \
                  "    if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {" \
                  "      console.error('‚úó Invalid SUPABASE_URL format');" \
                  "      process.exit(1);" \
                  "    }" \
                  "" \
                  "    const cleanUrl = supabaseUrl.replace(/\\/$/, '');" \
                  "    const supabase = createClient(cleanUrl, supabaseKey);" \
                  "" \
                  "    console.log('‚úì Getting admin user for Edge Function authentication...');" \
                  "" \
                  "    const { data: adminUsers, error: adminError } = await supabase" \
                  "      .from('system_users')" \
                  "      .select('user_id, email, status, role_id')" \
                  "      .eq('status', 'active')" \
                  "      .limit(1);" \
                  "" \
                  "    let userId = null;" \
                  "" \
                  "    if (!adminError && adminUsers && adminUsers.length > 0) {" \
                  "      const { data: roles } = await supabase.from('roles').select('role_id, role_name, permissions');" \
                  "      if (roles) {" \
                  "        const adminRoleIds = roles.filter(r => {" \
                  "          const perms = (r.permissions || '').toLowerCase();" \
                  "          const name = (r.role_name || '').toLowerCase();" \
                  "          return perms === 'all' || name.includes('admin') || name.includes('super');" \
                  "        }).map(r => r.role_id);" \
                  "        const adminUser = adminUsers.find(u => adminRoleIds.includes(u.role_id));" \
                  "        userId = adminUser ? adminUser.user_id : adminUsers[0].user_id;" \
                  "        console.log('‚úì Found user:', adminUser ? adminUser.email : adminUsers[0].email);" \
                  "      } else {" \
                  "        userId = adminUsers[0].user_id;" \
                  "        console.log('‚úì Using active user:', adminUsers[0].email);" \
                  "      }" \
                  "    }" \
                  "" \
                  "    if (!userId) throw new Error('No active admin user found');" \
                  "" \
                  "    console.log('‚úì Calling export-database Edge Function...');" \
                  "    const functionUrl = cleanUrl + '/functions/v1/export-database';" \
                  "    console.log('  ‚Üí Function URL:', functionUrl);" \
                  "" \
                  "    const response = await fetch(functionUrl, {" \
                  "      method: 'POST'," \
                  "      headers: {" \
                  "        'Content-Type': 'application/json'," \
                  "        'Authorization': 'Bearer ' + supabaseKey," \
                  "        'apikey': supabaseKey," \
                  "      }," \
                  "      body: JSON.stringify({ user_id: userId })" \
                  "    });" \
                  "" \
                  "    if (!response.ok) {" \
                  "      const errorText = await response.text();" \
                  "      throw new Error('Edge Function returned ' + response.status + ': ' + errorText);" \
                  "    }" \
                  "" \
                  "    const result = await response.json();" \
                  "    if (result.error || !result.success) {" \
                  "      throw new Error(result.error?.message || result.message || 'Export failed');" \
                  "    }" \
                  "" \
                  "    if (!result.sql_base64) throw new Error('Edge Function returned no SQL data');" \
                  "" \
                  "    const sql = Buffer.from(result.sql_base64, 'base64').toString('utf-8');" \
                  "    if (!sql || sql.length === 0) throw new Error('Decoded SQL is empty');" \
                  "" \
                  "    fs.writeFileSync('backup/db/backup.sql', sql);" \
                  "    console.log('‚úì Database exported via Edge Function');" \
                  "    console.log('  - Tables exported:', result.tables_exported, '/', result.tables_total || '?');" \
                  "    console.log('  - SQL size:', (result.sql_size / 1024).toFixed(2), 'KB');" \
                  "  } catch (err) {" \
                  "    console.error('‚úó Failed to export via Edge Function:', err.message);" \
                  "    const note = '-- ‚ö†Ô∏è DATABASE BACKUP FAILED\\n-- Error: ' + err.message + '\\n';" \
                  "    fs.writeFileSync('backup/db/backup.sql', note);" \
                  "    console.error('‚úì Created backup note file');" \
                  "    process.exit(0);" \
                  "  }" \
                  "})();" > /tmp/export-db.js
                
                echo "Running Edge Function export script..."
                if node --input-type=commonjs /tmp/export-db.js 2>&1; then
                  EDGE_FUNC_EXIT=$?
                  echo "Edge Function script completed with exit code: $EDGE_FUNC_EXIT"
                else
                  EDGE_FUNC_EXIT=$?
                  echo "Edge Function script failed with exit code: $EDGE_FUNC_EXIT"
                fi
                
                if [ -f backup/db/backup.sql ] && [ -s backup/db/backup.sql ]; then
                  echo "‚úì Database backup completed via Edge Function"
                  echo "‚úì File size: $(du -h backup/db/backup.sql | cut -f1)"
                  echo "‚úì Line count: $(wc -l < backup/db/backup.sql) lines"
                else
                  echo "‚ùå Database backup via Edge Function FAILED"
                  echo "‚ùå This is a critical error - database backup is required"
                  if [ -f backup/db/backup.sql ]; then
                    echo "‚ö†Ô∏è File exists but is empty or invalid"
                    echo "‚ö†Ô∏è File size: $(stat -f%z backup/db/backup.sql 2>/dev/null || stat -c%s backup/db/backup.sql 2>/dev/null || echo 'unknown') bytes"
                    echo "‚ö†Ô∏è First 10 lines of file:"
                    head -n 10 backup/db/backup.sql || echo "Could not read file"
                  else
                    echo "‚ö†Ô∏è Backup file was not created"
                    echo "‚ö†Ô∏è Checking if script file exists..."
                    if [ -f /tmp/export-db.js ]; then
                      echo "‚úì Script file exists"
                      echo "‚ö†Ô∏è First 20 lines of script:"
                      head -n 20 /tmp/export-db.js
                    else
                      echo "‚úó Script file was not created"
                    fi
                  fi
                  exit 1
                fi
              else
                echo "‚ùå pg_dump failed with error:"
                echo "$PG_DUMP_ERROR"
                echo ""
                echo "Common issues:"
                echo "1. Connection string format incorrect"
                echo "2. Database password incorrect"
                echo "3. Using pooled connection (port 6543) instead of direct (port 5432)"
                echo "4. IP not whitelisted (Supabase free plan may require IP whitelisting)"
                echo ""
                echo "Verify SUPABASE_DB_URL format:"
                echo "Correct: postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres"
                echo "Wrong:   postgresql://postgres:password@aws-1-eu-west-1.pooler.supabase.com:6543/postgres"
                # Exit with error for other issues (not network)
                exit 1
              fi
            }
          
          if [ -f backup/db/backup.sql ] && [ -s backup/db/backup.sql ]; then
            echo "‚úì Database export completed successfully"
          else
            echo "‚ö†Ô∏è Database export file is empty or missing"
          fi

      - name: Export auth users and system_users
        continue-on-error: true  # ‚úÖ Continue even if auth export fails (app uses custom auth)
        run: |
          mkdir -p backup/auth
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó Missing Supabase credentials');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          // ‚úÖ FIX: Export system_users table data (custom auth)
          console.log('‚úì Exporting system_users table (custom authentication)...');
          
          let supabaseAuthUsers = [];
          let systemUsers = [];
          
          // First, try to export Supabase Auth users (if any)
          supabase.auth.admin.listUsers()
            .then(({ data, error }) => {
              if (!error && data && data.users) {
                supabaseAuthUsers = data.users;
                console.log('‚úì Found', data.users.length, 'Supabase Auth users');
              } else {
                console.log('‚Ñπ No Supabase Auth users (using custom auth)');
              }
              
              // Now export system_users table
              return supabase
                .from('system_users')
                .select('*')
                .order('created_at', { ascending: false });
            })
            .then(({ data: systemUsersData, error: systemUsersError }) => {
              if (systemUsersError) {
                console.error('‚úó Error fetching system_users:', systemUsersError.message);
                throw systemUsersError;
              }
              
              if (!systemUsersData || systemUsersData.length === 0) {
                console.log('‚Ñπ No users found in system_users table');
                systemUsers = [];
              } else {
                systemUsers = systemUsersData;
                console.log('‚úì Found', systemUsersData.length, 'users in system_users table');
              }
              
              // ‚úÖ FIX: Create users.json with both Supabase Auth users and system_users
              const usersExport = {
                exported_at: new Date().toISOString(),
                supabase_auth_users: supabaseAuthUsers,
                system_users: systemUsers,
                total_users: supabaseAuthUsers.length + systemUsers.length,
                note: systemUsers.length > 0 
                  ? 'Users exported from system_users table (custom authentication system)'
                  : 'No users found in system_users table'
              };
              
              fs.writeFileSync(
                'backup/auth/users.json',
                JSON.stringify(usersExport, null, 2)
              );
              
              console.log('‚úì Exported users to backup/auth/users.json');
              console.log('  - Supabase Auth users:', supabaseAuthUsers.length);
              console.log('  - System users (system_users table):', systemUsers.length);
              console.log('  - Total users:', usersExport.total_users);
              
              process.exit(0);
            })
            .catch((err) => {
              // If Supabase Auth fails, still try to export system_users
              console.log('‚Ñπ Supabase Auth not accessible, exporting system_users only...');
              
              supabase
                .from('system_users')
                .select('*')
                .order('created_at', { ascending: false })
                .then(({ data: systemUsersData, error: systemUsersError }) => {
                  if (systemUsersError) {
                    console.error('‚úó Error fetching system_users:', systemUsersError.message);
                    throw systemUsersError;
                  }
                  
                  const systemUsers = systemUsersData || [];
                  console.log('‚úì Found', systemUsers.length, 'users in system_users table');
                  
                  const usersExport = {
                    exported_at: new Date().toISOString(),
                    supabase_auth_users: [],
                    system_users: systemUsers,
                    total_users: systemUsers.length,
                    note: systemUsers.length > 0 
                      ? 'Users exported from system_users table (custom authentication system). Supabase Auth not accessible.'
                      : 'No users found in system_users table'
                  };
                  
                  fs.writeFileSync(
                    'backup/auth/users.json',
                    JSON.stringify(usersExport, null, 2)
                  );
                  
                  console.log('‚úì Exported', systemUsers.length, 'system users to backup/auth/users.json');
                  process.exit(0);
                })
                .catch((finalErr) => {
                  console.error('‚úó Failed to export users:', finalErr.message);
                  
                  // Create a note file as fallback
                  fs.writeFileSync(
                    'backup/auth/users.json',
                    JSON.stringify({
                      exported_at: new Date().toISOString(),
                      supabase_auth_users: [],
                      system_users: [],
                      total_users: 0,
                      error: finalErr.message,
                      note: 'Failed to export users. Check database connection and permissions.'
                    }, null, 2)
                  );
                  console.log('‚úì Created users.json with error note');
                  process.exit(0); // Don't fail the workflow
                });
            });
          "

      - name: Download Supabase Storage files
        continue-on-error: false  # ‚úÖ Fail if storage backup fails (important data)
        run: |
          mkdir -p backup/storage
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          const path = require('path');
          
          (async () => {
            try {
              const supabaseUrl = process.env.SUPABASE_URL;
              const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
              
              // ‚úÖ FIX: Auto-detect buckets if SUPABASE_BUCKETS_TO_BACKUP is not set
              let buckets = (process.env.SUPABASE_BUCKETS_TO_BACKUP || '')
                .split(',')
                .map(b => b.trim())
                .filter(Boolean);
              
              if (!supabaseUrl || !supabaseKey) {
                console.error('‚úó Missing Supabase credentials');
                process.exit(1);
              }
              
              // ‚úÖ FIX: Validate URL format
              if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
                console.error('‚úó Invalid SUPABASE_URL format');
                console.error('URL must start with http:// or https://');
                console.error('Current value preview:', supabaseUrl.substring(0, 50));
                console.error('Expected format: https://xxxxx.supabase.co');
                process.exit(1);
              }
              
              const cleanUrl = supabaseUrl.replace(/\/$/, '');
              const supabase = createClient(cleanUrl, supabaseKey);
              
              // ‚úÖ FIX: Auto-detect buckets if not specified
              if (buckets.length === 0) {
                console.log('‚Ñπ SUPABASE_BUCKETS_TO_BACKUP not set, auto-detecting buckets...');
                try {
                  const { data: allBuckets, error: listError } = await supabase.storage.listBuckets();
              if (listError) {
                console.error('‚úó Error listing buckets:', listError.message);
                throw listError;
              }
              
              if (allBuckets && allBuckets.length > 0) {
                buckets = allBuckets.map(b => b.id).filter(id => id !== 's3'); // Exclude 's3' bucket name (it's a metadata identifier)
                console.log('‚úì Auto-detected buckets:', buckets.join(', '));
              } else {
                console.log('‚Ñπ No Supabase Storage buckets found');
                // Create a note about S3 storage
                fs.writeFileSync(
                  'backup/storage/note.txt',
                  'No Supabase Storage buckets found.\\n' +
                  'If your files are stored in AWS S3, they are not included in this backup.\\n' +
                  'S3 files should be backed up separately using AWS backup tools or S3 lifecycle policies.\\n' +
                  'File metadata (file_metadata table) is included in the database backup.'
                );
                console.log('‚úì Created storage backup note');
                process.exit(0);
              }
            } catch (detectErr) {
              console.error('‚úó Error auto-detecting buckets:', detectErr.message);
              // Create a note file
              fs.writeFileSync(
                'backup/storage/note.txt',
                'Could not detect storage buckets.\\n' +
                'Error: ' + detectErr.message + '\\n' +
                'Set SUPABASE_BUCKETS_TO_BACKUP secret in GitHub Settings to specify buckets manually.\\n' +
                'Example: profile-pictures,contracts,inventory,employees,branding,payroll,assets,custody'
              );
              process.exit(0); // Don't fail the workflow
            }
          } else {
            console.log('‚úì Using specified buckets:', buckets.join(', '));
          }
          
          async function downloadBucket(bucketName) {
            try {
              console.log('  ‚Üí Listing files in bucket:', bucketName);
              const { data: files, error } = await supabase.storage
                .from(bucketName)
                .list('', { limit: 1000, recursive: true });
              
              if (error) {
                console.error('  ‚úó Error listing files in', bucketName, ':', error.message);
                return { bucket: bucketName, downloaded: 0, error: error.message };
              }
              
              if (!files || files.length === 0) {
                console.log('  ‚Ñπ No files in bucket:', bucketName);
                return { bucket: bucketName, downloaded: 0 };
              }
              
              console.log('  ‚Üí Found', files.length, 'files in', bucketName);
              const bucketDir = path.join('backup/storage', bucketName);
              fs.mkdirSync(bucketDir, { recursive: true });
              
              let downloaded = 0;
              let failed = 0;
              
              for (const file of files) {
                // Skip directories (they don't have an id)
                if (!file.id || file.name.endsWith('/')) {
                  continue;
                }
                
                try {
                  const { data, error: downloadError } = await supabase.storage
                    .from(bucketName)
                    .download(file.name);
                  
                  if (downloadError) {
                    console.error('    ‚úó Error downloading', file.name, ':', downloadError.message);
                    failed++;
                    continue;
                  }
                  
                  const filePath = path.join(bucketDir, file.name);
                  const dir = path.dirname(filePath);
                  fs.mkdirSync(dir, { recursive: true });
                  fs.writeFileSync(filePath, Buffer.from(await data.arrayBuffer()));
                  downloaded++;
                  
                  if (downloaded % 10 === 0) {
                    console.log('    ‚Üí Downloaded', downloaded, 'files...');
                  }
                } catch (fileErr) {
                  console.error('    ‚úó Error processing', file.name, ':', fileErr.message);
                  failed++;
                }
              }
              
              console.log('  ‚úì Downloaded', downloaded, 'files from', bucketName, (failed > 0 ? '(failed: ' + failed + ')' : ''));
              return { bucket: bucketName, downloaded, failed };
            } catch (bucketErr) {
              console.error('  ‚úó Error backing up bucket', bucketName, ':', bucketErr.message);
              return { bucket: bucketName, downloaded: 0, error: bucketErr.message };
            }
          }
          
          Promise.all(buckets.map(downloadBucket))
            .then((results) => {
              const totalDownloaded = results.reduce((sum, r) => sum + (r.downloaded || 0), 0);
              const totalFailed = results.reduce((sum, r) => sum + (r.failed || 0), 0);
              
              console.log('‚úì Storage backup completed');
              console.log('  - Buckets processed:', buckets.length);
              console.log('  - Total files downloaded:', totalDownloaded);
              if (totalFailed > 0) {
                console.log('  ‚ö†Ô∏è Failed downloads:', totalFailed);
              }
              
              // Check if any buckets had errors
              const bucketsWithErrors = results.filter(r => r.error);
              if (bucketsWithErrors.length > 0) {
                const errorBuckets = bucketsWithErrors.map(r => r.bucket).join(', ');
                const errorDetails = bucketsWithErrors.map(r => r.bucket + ' (' + r.error + ')').join(', ');
                console.error('  ‚úó Errors in buckets:', errorBuckets);
                throw new Error('Failed to backup ' + bucketsWithErrors.length + ' bucket(s): ' + errorDetails);
              }
              
              // Create a summary file
              fs.writeFileSync(
                'backup/storage/backup_summary.json',
                JSON.stringify({
                  exported_at: new Date().toISOString(),
                  buckets_backed_up: buckets,
                  results: results,
                  total_files_downloaded: totalDownloaded,
                  total_failed: totalFailed,
                  note: 'Supabase Storage buckets backed up. S3 files are backed up in a separate step (backup/storage/s3/).'
                }, null, 2)
              );
              
              if (totalDownloaded === 0 && buckets.length > 0) {
                console.log('  ‚ÑπÔ∏è No files found in any buckets (this may be normal if buckets are empty)');
              }
            })
            .catch((err) => {
              console.error('‚úó Failed to download storage files:', err);
              console.error('‚úó Error details:', err.message);
              // Create error note
              fs.writeFileSync(
                'backup/storage/error_note.txt',
                'Storage backup failed: ' + err.message + '\\n' +
                'File metadata is still available in the database backup (file_metadata table).'
              );
              throw err; // Fail the workflow
            });
            } catch (err) {
              console.error('‚úó Failed to download storage files:', err);
              console.error('‚úó Error details:', err.message);
              fs.writeFileSync(
                'backup/storage/error_note.txt',
                'Storage backup failed: ' + err.message + '\\n' +
                'File metadata is still available in the database backup (file_metadata table).'
              );
              process.exit(1);
            }
          })();
          "

      - name: Download S3 Storage files
        continue-on-error: true  # ‚úÖ Continue even if S3 backup fails
        run: |
          mkdir -p backup/storage/s3
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          const { S3Client, GetObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const path = require('path');
          
          (async () => {
            try {
              const supabaseUrl = process.env.SUPABASE_URL;
              const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
              const awsAccessKeyId = process.env.AWS_ACCESS_KEY_ID;
              const awsSecretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;
              const awsRegion = process.env.AWS_S3_REGION || 'us-east-1';
              const awsBucket = process.env.AWS_S3_BUCKET;
              const backupS3Files = process.env.BACKUP_S3_FILES !== 'false';
              
              if (!backupS3Files) {
                console.log('‚Ñπ S3 file backup is disabled (BACKUP_S3_FILES=false)');
                fs.writeFileSync('backup/storage/s3/note.txt', 'S3 file backup is disabled.\\nSet BACKUP_S3_FILES=true in GitHub Secrets to enable S3 file backup.');
                process.exit(0);
              }
              
              if (!supabaseUrl || !supabaseKey) {
                console.error('‚úó Missing Supabase credentials');
                process.exit(1);
              }
              
              if (!awsAccessKeyId || !awsSecretAccessKey || !awsBucket) {
                console.log('‚Ñπ AWS S3 credentials not configured');
                fs.writeFileSync('backup/storage/s3/note.txt', 'AWS S3 credentials not configured. File metadata is still included in the database backup.');
                process.exit(0);
              }
              
              if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
                console.error('‚úó Invalid SUPABASE_URL format');
                process.exit(1);
              }
              
              const cleanUrl = supabaseUrl.replace(/\/\$/, '');
              const supabase = createClient(cleanUrl, supabaseKey);
              const s3Client = new S3Client({
                region: awsRegion,
                credentials: { accessKeyId: awsAccessKeyId, secretAccessKey: awsSecretAccessKey },
              });
              
              console.log('‚úì Fetching S3 file list from file_metadata table...');
              
              let offset = 0;
              const limit = 1000;
              let allFiles = [];
              let hasMore = true;
              
              while (hasMore) {
                const { data: files, error } = await supabase
                  .from('file_metadata')
                  .select('id, path, file_name, mime_type, size, category, owner_id, owner_type, created_at')
                  .eq('bucket', 's3')
                  .is('deleted_at', null)
                  .range(offset, offset + limit - 1)
                  .order('created_at', { ascending: false });
                
                if (error) {
                  console.error('‚úó Error fetching file metadata:', error.message);
                  throw error;
                }
                
                if (!files || files.length === 0) {
                  hasMore = false;
                  break;
                }
                
                allFiles = allFiles.concat(files);
                offset += limit;
                hasMore = files.length === limit;
              }
              
              if (allFiles.length === 0) {
                console.log('‚Ñπ No S3 files found in file_metadata table');
                fs.writeFileSync('backup/storage/s3/note.txt', 'No S3 files found in file_metadata table.');
                process.exit(0);
              }
              
              console.log('‚úì Found', allFiles.length, 'S3 files to backup');
              let downloaded = 0;
              let failed = 0;
              const failedFiles = [];
              
              for (const file of allFiles) {
                try {
                  const getObjectCommand = new GetObjectCommand({ Bucket: awsBucket, Key: file.path });
                  const response = await s3Client.send(getObjectCommand);
                  const chunks = [];
                  for await (const chunk of response.Body) chunks.push(chunk);
                  const fileBuffer = Buffer.concat(chunks);
                  const backupPath = path.join('backup/storage/s3', file.path);
                  fs.mkdirSync(path.dirname(backupPath), { recursive: true });
                  fs.writeFileSync(backupPath, fileBuffer);
                  downloaded++;
                  if (downloaded % 10 === 0) console.log('  ‚Üí Downloaded', downloaded, '/', allFiles.length, 'files...');
                } catch (fileErr) {
                  failed++;
                  failedFiles.push({ path: file.path, file_name: file.file_name, error: fileErr.message });
                  console.error('  ‚úó Failed to download', file.path, ':', fileErr.message);
                }
              }
              
              console.log('‚úì S3 backup completed');
              console.log('  - Files downloaded:', downloaded, '/', allFiles.length);
              if (failed > 0) console.log('  - Failed downloads:', failed);
              
              fs.writeFileSync('backup/storage/s3/backup_summary.json', JSON.stringify({
                exported_at: new Date().toISOString(),
                total_files: allFiles.length,
                downloaded: downloaded,
                failed: failed,
                s3_bucket: awsBucket,
                s3_region: awsRegion,
                failed_files: failedFiles
              }, null, 2));
              
              if (failed > 0) {
                fs.writeFileSync('backup/storage/s3/failed_files.json', JSON.stringify(failedFiles, null, 2));
                console.log('  ‚ö† Created failed_files.json with', failed, 'failed downloads');
              }
            } catch (err) {
              console.error('‚úó S3 backup failed:', err.message);
              fs.writeFileSync('backup/storage/s3/error_note.txt', 'S3 backup failed: ' + err.message);
              process.exit(0);
            }
          })();
          "

      - name: Create backup archive
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%d-%H-%M-UTC")
          BACKUP_NAME="backup-$TIMESTAMP.zip"
          zip -r "$BACKUP_NAME" backup/
          echo "BACKUP_FILE=$BACKUP_NAME" >> $GITHUB_ENV
          echo "BACKUP_SIZE=$(stat -f%z "$BACKUP_NAME" 2>/dev/null || stat -c%s "$BACKUP_NAME")" >> $GITHUB_ENV
          echo "‚úì Created backup archive: $BACKUP_NAME"

      - name: Upload to S3
        run: |
          S3_PATH="backups/$(date -u +"%Y/%m/%d")/${{ env.BACKUP_FILE }}"
          aws s3 cp "${{ env.BACKUP_FILE }}" "s3://$AWS_S3_BUCKET/$S3_PATH" \
            --region "$AWS_S3_REGION"
          echo "S3_KEY=$S3_PATH" >> $GITHUB_ENV
          echo "‚úì Uploaded to S3: $S3_PATH"

      - name: Update backup_history and system_settings
        # ‚úÖ FIX: Pass variables via command line, not export
        # Variables from env: are already available, but we pass STATUS/ERROR_TEXT explicitly
        run: |
          # Check if database backup failed (network issue)
          if [ -f backup/db/backup.sql ] && grep -q "DATABASE BACKUP FAILED" backup/db/backup.sql; then
            FINAL_STATUS="partial"
            FINAL_ERROR_TEXT="Database backup failed: Network unreachable (Supabase Free Plan blocks external connections). Auth users and storage files backed up successfully. Enable IP whitelisting or upgrade to Pro plan for complete backup."
          else
            FINAL_STATUS="success"
            FINAL_ERROR_TEXT=""
          fi
          
          S3_KEY="${{ env.S3_KEY }}"
          BACKUP_SIZE="${{ env.BACKUP_SIZE }}"
          
          # ‚úÖ FIX: Pass variables directly to Node.js process
          STATUS="$FINAL_STATUS" \
          ERROR_TEXT="$FINAL_ERROR_TEXT" \
          S3_KEY="$S3_KEY" \
          BACKUP_SIZE="$BACKUP_SIZE" \
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          
          // ‚úÖ FIX: Get all variables from process.env
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const dispatchId = process.env.DISPATCH_ID;
          const status = process.env.STATUS;
          const errorText = process.env.ERROR_TEXT || null;
          const s3Key = process.env.S3_KEY;
          const sizeBytes = process.env.BACKUP_SIZE ? parseInt(process.env.BACKUP_SIZE, 10) : null;
          
          // ‚úÖ FIX: Validate all required variables
          if (!supabaseUrl || !supabaseKey) {
            console.error('‚úó CRITICAL ERROR: Missing Supabase credentials');
            console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
            console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó CRITICAL ERROR: Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            console.error('');
            console.error('Please check your VITE_SUPABASE_URL secret in GitHub Settings');
            console.error('It should be your Supabase project URL, NOT the database connection string');
            console.error('Correct: https://rqssjgiunwyjeyutgkkp.supabase.co');
            console.error('Wrong:   postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          if (!dispatchId) {
            console.error('‚úó CRITICAL ERROR: Missing DISPATCH_ID');
            process.exit(1);
          }
          
          console.log('‚úì Supabase credentials validated');
          console.log('‚úì URL format validated');
          console.log('‚úì Updating backup_history with:');
          console.log('  - dispatch_id:', dispatchId);
          console.log('  - status:', status);
          console.log('  - s3_key:', s3Key);
          console.log('  - size_bytes:', sizeBytes);
          
          const supabase = createClient(cleanUrl, supabaseKey);
          
          // Update backup_history
          supabase
            .from('backup_history')
            .update({
              status: status,
              s3_key: s3Key,
              size_bytes: sizeBytes,
              error_text: errorText,
            })
            .eq('dispatch_id', dispatchId)
            .then(({ data, error }) => {
              if (error) {
                console.error('‚úó CRITICAL ERROR updating backup history');
                console.error('Error:', error);
                console.error('Error code:', error.code);
                console.error('Error message:', error.message);
                console.error('Error details:', error.details);
                console.error('Error hint:', error.hint);
                process.exit(1);
              }
              
              console.log('‚úì Backup history updated successfully');
              
              // Update system_settings_kv
              return supabase
                .from('system_settings_kv')
                .upsert({
                  key: 'last_backup_at',
                  value: new Date().toISOString(),
                }, {
                  onConflict: 'key',
                });
            })
            .then(({ error: settingsError }) => {
              if (settingsError) {
                console.error('‚ö† Warning: Failed to update last_backup_at:', settingsError.message);
                // Don't fail the step, just log the warning
              } else {
                console.log('‚úì System settings updated');
              }
            })
            .catch((err) => {
              console.error('‚úó CRITICAL ERROR:', err);
              process.exit(1);
            });
          "

      - name: Handle backup failure
        if: failure()
        # ‚úÖ FIX: Use the same secret names as job-level env
        env:
          SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
        run: |
          FINAL_STATUS="failed"
          FINAL_ERROR_TEXT="Backup workflow failed. Check GitHub Actions logs for details."
          
          # Debug: Check if variables are available
          echo "Debug: Checking environment variables in failure handler..."
          echo "SUPABASE_URL: ${SUPABASE_URL:+SET (hidden)}${SUPABASE_URL:-MISSING}"
          echo "SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY:+SET (hidden)}${SUPABASE_SERVICE_ROLE_KEY:-MISSING}"
          echo "DISPATCH_ID: $DISPATCH_ID"
          
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] || [ -z "$DISPATCH_ID" ]; then
            echo "‚ùå Cannot update backup status - missing required variables"
            echo "This is a critical error. Please check GitHub Secrets configuration."
            exit 1
          fi
          
          STATUS="$FINAL_STATUS" \
          ERROR_TEXT="$FINAL_ERROR_TEXT" \
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const dispatchId = process.env.DISPATCH_ID;
          const status = process.env.STATUS;
          const errorText = process.env.ERROR_TEXT;
          
          if (!supabaseUrl || !supabaseKey || !dispatchId) {
            console.error('‚úó Cannot update backup status - missing credentials');
            console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
            console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
            console.error('DISPATCH_ID:', dispatchId ? 'SET' : 'MISSING');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('‚úó Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          // ‚úÖ FIX: Remove trailing slash if present
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          console.log('‚úì Updating backup status to failed');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('backup_history')
            .update({
              status: status,
              error_text: errorText,
            })
            .eq('dispatch_id', dispatchId)
            .then(({ error }) => {
              if (error) {
                console.error('‚úó Failed to update backup status to failed:', error.message);
                process.exit(1);
              } else {
                console.log('‚úì Backup status updated to failed');
              }
            })
            .catch((err) => {
              console.error('‚úó Error updating backup status:', err);
              process.exit(1);
            });
          "
