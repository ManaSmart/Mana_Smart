name: Backup Database and Storage
run-name: >
  Backup â€“ ${{ github.event.inputs.trigger_type || 'auto' }} â€“
  ${{ github.event.inputs.dispatch_id || github.run_id }}

on:
  workflow_dispatch:  # Allow manual triggers
    inputs:
      dispatch_id:
        description: 'Unique dispatch ID for tracking this backup'
        required: false
        type: string
      trigger_type:
        description: 'Type of trigger (manual or scheduled)'
        required: false
        type: string
        default: 'manual'
  schedule:
    - cron: '0 2 * * *'  # Daily at 2:00 AM UTC

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    # âœ… FIX: Define all environment variables at job level
    # This makes them available to ALL steps
    # Note: Using VITE_ prefixed secrets to match current GitHub Secrets configuration
    env:
      SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
      SUPABASE_DB_URL: ${{ secrets.DATABASE_URL }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_S3_REGION: ${{ secrets.AWS_S3_REGION }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SUPABASE_BUCKETS_TO_BACKUP: ${{ secrets.SUPABASE_BUCKETS_TO_BACKUP }}
      DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
      TRIGGER_TYPE: ${{ github.event.inputs.trigger_type || 'scheduled' }}
      BACKUP_S3_FILES: ${{ secrets.BACKUP_S3_FILES || 'true' }}
      BACKUP_API_KEY: ${{ secrets.BACKUP_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        # âœ… FIX: Disable submodule checkout to avoid the warning
        with:
          submodules: false

      - name: Debug environment variables
        # âœ… DEBUG: Check if secrets are available (without exposing values)
        run: |
          echo "Checking environment variables..."
          echo "SUPABASE_URL: ${SUPABASE_URL:+SET (hidden)}${SUPABASE_URL:-MISSING}"
          echo "SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY:+SET (hidden)}${SUPABASE_SERVICE_ROLE_KEY:-MISSING}"
          echo "SUPABASE_DB_URL: ${SUPABASE_DB_URL:+SET (hidden)}${SUPABASE_DB_URL:-MISSING}"
          echo "AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:+SET (hidden)}${AWS_ACCESS_KEY_ID:-MISSING}"
          echo "AWS_S3_BUCKET: ${AWS_S3_BUCKET:+SET (hidden)}${AWS_S3_BUCKET:-MISSING}"
          echo "DISPATCH_ID: $DISPATCH_ID"
          
          # Check if secrets are actually set
          if [ -z "$SUPABASE_URL" ]; then
            echo "âŒ ERROR: VITE_SUPABASE_URL secret is not set in GitHub Secrets"
            echo "Please go to: Repository â†’ Settings â†’ Secrets and variables â†’ Actions"
            echo "And add VITE_SUPABASE_URL secret"
            exit 1
          fi
          
          if [ -z "$SUPABASE_SERVICE_ROLE_KEY" ]; then
            echo "âŒ ERROR: VITE_SUPABASE_SERVICE_ROLE_KEY secret is not set in GitHub Secrets"
            echo "Please go to: Repository â†’ Settings â†’ Secrets and variables â†’ Actions"
            echo "And add VITE_SUPABASE_SERVICE_ROLE_KEY secret"
            exit 1
          fi
          
          echo "âœ“ All required secrets are set"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Update workflow_run_id in backup_history
        # âœ… NEW: Update backup_history with workflow_run_id for progress tracking
        # âœ… FIX: Run after npm install so @supabase/supabase-js is available
        run: |
          DISPATCH_ID="${{ github.event.inputs.dispatch_id || github.run_id }}"
          WORKFLOW_RUN_ID="${{ github.run_id }}"
          
          node --input-type=commonjs -e "
          (async () => {
            try {
              const { createClient } = require('@supabase/supabase-js');
              
              const supabaseUrl = process.env.SUPABASE_URL;
              const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
              const dispatchId = process.env.DISPATCH_ID;
              const workflowRunId = process.env.WORKFLOW_RUN_ID;
              
              if (!supabaseUrl || !supabaseKey || !dispatchId || !workflowRunId) {
                console.log('âš  Skipping workflow_run_id update - missing variables');
                process.exit(0);
              }
              
              const cleanUrl = supabaseUrl.replace(/\\/$/, '');
              const supabase = createClient(cleanUrl, supabaseKey);
              
              // Update backup_history with workflow_run_id
              const { error } = await supabase
                .from('backup_history')
                .update({ workflow_run_id: workflowRunId })
                .eq('dispatch_id', dispatchId);
              
              if (error) {
                console.log('âš  Could not update workflow_run_id:', error.message);
              } else {
                console.log('âœ“ Updated workflow_run_id:', workflowRunId);
              }
            } catch (err) {
              console.log('âš  Error updating workflow_run_id:', err.message);
              // Don't fail the workflow if this step fails
            }
          })();
          "
        env:
          SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
          WORKFLOW_RUN_ID: ${{ github.run_id }}

      - name: Check backup enable status
        # âœ… FIX: Environment variables are inherited from job-level env:
        run: |
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          
          // âœ… FIX: Get from process.env (automatically available from job env:)
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          // âœ… DEBUG: Validate URL format (without exposing full value)
          console.log('Validating Supabase URL...');
          console.log('URL length:', supabaseUrl ? supabaseUrl.length : 0);
          console.log('URL starts with https://', supabaseUrl ? supabaseUrl.startsWith('https://') : false);
          console.log('URL preview:', supabaseUrl ? supabaseUrl.substring(0, 30) + '...' : 'MISSING');
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('âœ— Missing Supabase credentials');
            console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
            console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
            process.exit(1);
          }
          
          // âœ… FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('âœ— Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value starts with:', supabaseUrl.substring(0, 20));
            console.error('Expected format: https://xxxxx.supabase.co');
            console.error('');
            console.error('Please check your VITE_SUPABASE_URL secret in GitHub Settings');
            console.error('It should be your Supabase project URL, not the database connection string');
            process.exit(1);
          }
          
          // âœ… FIX: Remove trailing slash if present
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          console.log('âœ“ Supabase credentials found');
          console.log('âœ“ URL format validated');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('system_settings_kv')
            .select('value')
            .eq('key', 'backup_enabled')
            .single()
            .then(({ data, error }) => {
              if (error && error.code !== 'PGRST116') {
                console.error('âœ— Error checking backup status:', error);
                process.exit(1);
              }
              
              const enabled = data?.value?.enabled ?? false;
              if (!enabled) {
                console.log('â„¹ Backup is disabled. Exiting.');
                process.exit(0);
              }
              
              console.log('âœ“ Backup is enabled. Proceeding...');
            })
            .catch((err) => {
              console.error('âœ— Failed to check backup status:', err);
              process.exit(1);
            });
          "

      - name: Update backup status (in progress)
        run: |
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          const dispatchId = process.env.DISPATCH_ID;
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('âœ— Missing Supabase credentials');
            process.exit(1);
          }
          
          // âœ… FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('âœ— Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          
          if (!dispatchId) {
            console.error('âœ— Missing DISPATCH_ID');
            process.exit(1);
          }
          
          console.log('âœ“ Updating backup status to in_progress');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          supabase
            .from('backup_history')
            .update({ status: 'in_progress' })
            .eq('dispatch_id', dispatchId)
            .then(({ error }) => {
              if (error) {
                console.error('âœ— Error updating backup status:', error);
                process.exit(1);
              }
              console.log('âœ“ Backup status updated to in_progress');
            })
            .catch((err) => {
              console.error('âœ— Failed to update backup status:', err);
              process.exit(1);
            });
          "

      - name: Export database
        continue-on-error: false  # âœ… Fail if database export fails (critical step)
        run: |
          mkdir -p backup/db
          
          # âœ… FIX: Verify connection string is set
          if [ -z "$SUPABASE_DB_URL" ]; then
            echo "âŒ ERROR: DATABASE_URL secret is not set"
            echo "Please set DATABASE_URL secret in GitHub Settings"
            echo "Format: postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres"
            exit 1
          fi
          
          echo "âœ“ Starting database export..."
          echo "Connection string format: ${SUPABASE_DB_URL%%@*}@***" # Show only user@host part
          
          # âœ… FIX: Try pg_dump, handle network errors gracefully
          echo "Attempting direct database connection via pg_dump..."
          set +e  # Don't exit on error, we'll handle it
          
          # Use a different approach to capture both output and exit code
          pg_dump "$SUPABASE_DB_URL" \
            --no-owner \
            --no-privileges \
            --format=plain \
            --blobs \
            --verbose \
            --file=backup/db/backup.sql > /tmp/pg_dump_output.log 2>&1
          PG_DUMP_EXIT=$?
          
          # Also save output for debugging
          cat /tmp/pg_dump_output.log | tee /dev/stderr
          
          echo ""
          echo "DEBUG: pg_dump exit code: $PG_DUMP_EXIT"
          echo "DEBUG: Checking if error handler should run..."
          
          if [ $PG_DUMP_EXIT -ne 0 ]; then
            echo "DEBUG: âœ“ Entering error handler for pg_dump failure"
            PG_DUMP_ERROR=$(cat /tmp/pg_dump_output.log)
            
            echo ""
            echo "DEBUG: pg_dump failed with exit code: $PG_DUMP_EXIT"
            echo "DEBUG: Error output saved to /tmp/pg_dump_output.log"
            echo "DEBUG: Checking error type..."
            echo "DEBUG: Error contains 'Network': $(echo "$PG_DUMP_ERROR" | grep -ci "Network" || echo "0")"
            echo "DEBUG: Error contains 'unreachable': $(echo "$PG_DUMP_ERROR" | grep -ci "unreachable" || echo "0")"
            echo "DEBUG: Full error for debugging:"
            echo "$PG_DUMP_ERROR" | head -n 5
            
            # Check if it's a network/unreachable error (Supabase free plan issue)
            # Match "Network is unreachable" (the exact error message)
            if echo "$PG_DUMP_ERROR" | grep -qi "Network is unreachable"; then
              NETWORK_MATCH="yes"
            elif echo "$PG_DUMP_ERROR" | grep -qi "unreachable"; then
              NETWORK_MATCH="yes"
            elif echo "$PG_DUMP_ERROR" | grep -qiE "(Connection refused|timeout|could not connect)"; then
              NETWORK_MATCH="yes"
            else
              NETWORK_MATCH="no"
            fi
            
            echo "DEBUG: Network error match result: $NETWORK_MATCH"
            
            if [ "$NETWORK_MATCH" = "yes" ]; then
                echo ""
                echo "âš ï¸ WARNING: Direct database connection failed (Network unreachable)"
                echo "This is common with Supabase Free Plan - external connections are blocked"
                echo ""
                echo "ðŸ”„ Attempting backup via Supabase Edge Function (bypasses IP restrictions)..."
                echo ""
                
                # âœ… FIX: Use Edge Function to export database via API
                # Write script to file using printf to avoid heredoc issues
                printf '%s\n' \
                  "const { createClient } = require('@supabase/supabase-js');" \
                  "const fs = require('fs');" \
                  "" \
                  "(async () => {" \
                  "  try {" \
                  "    const supabaseUrl = process.env.SUPABASE_URL;" \
                  "    const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;" \
                  "" \
                  "    if (!supabaseUrl || !supabaseKey) {" \
                  "      console.error('âœ— Cannot use Edge Function export - missing credentials');" \
                  "      process.exit(1);" \
                  "    }" \
                  "" \
                  "    if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {" \
                  "      console.error('âœ— Invalid SUPABASE_URL format');" \
                  "      process.exit(1);" \
                  "    }" \
                  "" \
                  "    const cleanUrl = supabaseUrl.replace(/\\/$/, '');" \
                  "    const supabase = createClient(cleanUrl, supabaseKey);" \
                  "" \
                  "    console.log('âœ“ Getting admin user for Edge Function authentication...');" \
                  "" \
                  "    const { data: adminUsers, error: adminError } = await supabase" \
                  "      .from('system_users')" \
                  "      .select('user_id, email, status, role_id')" \
                  "      .eq('status', 'active')" \
                  "      .limit(1);" \
                  "" \
                  "    let userId = null;" \
                  "" \
                  "    if (!adminError && adminUsers && adminUsers.length > 0) {" \
                  "      const { data: roles } = await supabase.from('roles').select('role_id, role_name, permissions');" \
                  "      if (roles) {" \
                  "        const adminRoleIds = roles.filter(r => {" \
                  "          // Handle permissions as string, object, or array" \
                  "          let permsStr = '';" \
                  "          if (typeof r.permissions === 'string') {" \
                  "            permsStr = r.permissions.toLowerCase();" \
                  "          } else if (r.permissions) {" \
                  "            permsStr = JSON.stringify(r.permissions).toLowerCase();" \
                  "          }" \
                  "          const name = (r.role_name || '').toLowerCase();" \
                  "          return permsStr === 'all' || permsStr.includes('\"all\"') || name.includes('admin') || name.includes('super');" \
                  "        }).map(r => r.role_id);" \
                  "        const adminUser = adminUsers.find(u => adminRoleIds.includes(u.role_id));" \
                  "        userId = adminUser ? adminUser.user_id : adminUsers[0].user_id;" \
                  "        console.log('âœ“ Found user:', adminUser ? adminUser.email : adminUsers[0].email);" \
                  "      } else {" \
                  "        userId = adminUsers[0].user_id;" \
                  "        console.log('âœ“ Using active user:', adminUsers[0].email);" \
                  "      }" \
                  "    }" \
                  "" \
                  "    if (!userId) throw new Error('No active admin user found');" \
                  "" \
                  "    console.log('âœ“ Calling export-database Edge Function...');" \
                  "    const functionUrl = cleanUrl + '/functions/v1/export-database';" \
                  "    console.log('  â†’ Function URL:', functionUrl);" \
                  "" \
                  "    const response = await fetch(functionUrl, {" \
                  "      method: 'POST'," \
                  "      headers: {" \
                  "        'Content-Type': 'application/json'," \
                  "        'Authorization': 'Bearer ' + supabaseKey," \
                  "        'apikey': supabaseKey," \
                  "      }," \
                  "      body: JSON.stringify({ user_id: userId })" \
                  "    });" \
                  "" \
                  "    if (!response.ok) {" \
                  "      const errorText = await response.text();" \
                  "      throw new Error('Edge Function returned ' + response.status + ': ' + errorText);" \
                  "    }" \
                  "" \
                  "    const result = await response.json();" \
                  "    if (result.error || !result.success) {" \
                  "      throw new Error(result.error?.message || result.message || 'Export failed');" \
                  "    }" \
                  "" \
                  "    if (!result.sql_base64) throw new Error('Edge Function returned no SQL data');" \
                  "" \
                  "    const sql = Buffer.from(result.sql_base64, 'base64').toString('utf-8');" \
                  "    if (!sql || sql.length === 0) throw new Error('Decoded SQL is empty');" \
                  "" \
                  "    fs.writeFileSync('backup/db/backup.sql', sql);" \
                  "    console.log('âœ“ Database exported via Edge Function');" \
                  "    console.log('  - Tables exported:', result.tables_exported, '/', result.tables_total || '?');" \
                  "    console.log('  - SQL size:', (result.sql_size / 1024).toFixed(2), 'KB');" \
                  "  } catch (err) {" \
                  "    console.error('âœ— Failed to export via Edge Function:', err.message);" \
                  "    const note = '-- âš ï¸ DATABASE BACKUP FAILED\\n-- Error: ' + err.message + '\\n';" \
                  "    fs.writeFileSync('backup/db/backup.sql', note);" \
                  "    console.error('âœ“ Created backup note file');" \
                  "    process.exit(0);" \
                  "  }" \
                  "})();" > ./export-db.cjs
                
                echo "Running Edge Function export script..."
                echo "DEBUG: Script file location: ./export-db.cjs"
                echo "DEBUG: Script file exists: $([ -f ./export-db.cjs ] && echo 'YES' || echo 'NO')"
                if [ -f ./export-db.cjs ]; then
                  echo "DEBUG: Script file size: $(wc -l < ./export-db.cjs) lines"
                  echo "DEBUG: Current directory: $(pwd)"
                  echo "DEBUG: node_modules exists: $([ -d ./node_modules ] && echo 'YES' || echo 'NO')"
                  echo "DEBUG: @supabase/supabase-js exists: $([ -d ./node_modules/@supabase/supabase-js ] && echo 'YES' || echo 'NO')"
                fi
                
                # Run the script from project root so it can find node_modules
                # Using .cjs extension forces CommonJS mode regardless of package.json type
                echo "=== Starting Edge Function Script ==="
                set +e  # Don't exit on error, we'll check manually
                node ./export-db.cjs
                EDGE_FUNC_EXIT=$?
                set -e  # Re-enable exit on error
                echo "=== Edge Function Script Finished ==="
                echo "Script exit code: $EDGE_FUNC_EXIT"
                
                # Clean up script file
                rm -f ./export-db.cjs
                
                # Check if backup file was created
                sleep 1  # Give file system a moment to sync
                if [ -f backup/db/backup.sql ]; then
                  FILE_SIZE=$(stat -f%z backup/db/backup.sql 2>/dev/null || stat -c%s backup/db/backup.sql 2>/dev/null || echo '0')
                  echo "DEBUG: Backup file exists, size: $FILE_SIZE bytes"
                  
                  if [ "$FILE_SIZE" -gt 100 ]; then
                    echo "âœ“ Database backup completed via Edge Function"
                    echo "âœ“ File size: $(du -h backup/db/backup.sql | cut -f1)"
                    echo "âœ“ Line count: $(wc -l < backup/db/backup.sql) lines"
                  else
                    echo "âš ï¸ Backup file exists but is very small ($FILE_SIZE bytes)"
                    echo "âš ï¸ First 20 lines of file:"
                    head -n 20 backup/db/backup.sql || echo "Could not read file"
                    echo "âŒ Database backup via Edge Function FAILED - file too small"
                    exit 1
                  fi
                else
                  echo "âŒ Database backup via Edge Function FAILED"
                  echo "âŒ Backup file was not created"
                  echo "DEBUG: Checking script file..."
                  if [ -f /tmp/export-db.js ]; then
                    echo "âœ“ Script file exists ($(wc -l < /tmp/export-db.js) lines)"
                    echo "DEBUG: First 30 lines of script:"
                    head -n 30 /tmp/export-db.js
                  else
                    echo "âœ— Script file was not created"
                  fi
                  exit 1
                fi
              else
                echo "âŒ pg_dump failed with error:"
                echo "$PG_DUMP_ERROR"
                echo ""
                echo "Common issues:"
                echo "1. Connection string format incorrect"
                echo "2. Database password incorrect"
                echo "3. Using pooled connection (port 6543) instead of direct (port 5432)"
                echo "4. IP not whitelisted (Supabase free plan may require IP whitelisting)"
                echo ""
                echo "Verify SUPABASE_DB_URL format:"
                echo "Correct: postgresql://postgres:password@db.xxxxx.supabase.co:5432/postgres"
                echo "Wrong:   postgresql://postgres:password@aws-1-eu-west-1.pooler.supabase.com:6543/postgres"
                # Exit with error for other issues (not network)
                exit 1
              fi
            fi
          
          # Final check - if file doesn't exist or is empty, it's an error
          # Only check if pg_dump succeeded (exit code 0)
          if [ $PG_DUMP_EXIT -eq 0 ]; then
            if [ -f backup/db/backup.sql ] && [ -s backup/db/backup.sql ]; then
              echo "âœ“ Database export completed successfully via pg_dump"
              FILE_SIZE=$(stat -f%z backup/db/backup.sql 2>/dev/null || stat -c%s backup/db/backup.sql 2>/dev/null || echo '0')
              if [ "$FILE_SIZE" -lt 100 ]; then
                echo "âš ï¸ WARNING: Backup file is very small ($FILE_SIZE bytes) - might be an error note"
                echo "âš ï¸ First 10 lines:"
                head -n 10 backup/db/backup.sql
              fi
            else
              echo "âŒ CRITICAL: pg_dump reported success but backup file is missing or empty"
              exit 1
            fi
          else
            # pg_dump failed - check if Edge Function fallback created the file
            if [ -f backup/db/backup.sql ] && [ -s backup/db/backup.sql ]; then
              FILE_SIZE=$(stat -f%z backup/db/backup.sql 2>/dev/null || stat -c%s backup/db/backup.sql 2>/dev/null || echo '0')
              if [ "$FILE_SIZE" -gt 100 ]; then
                echo "âœ“ Database export completed via Edge Function fallback"
                echo "âœ“ File size: $(du -h backup/db/backup.sql | cut -f1)"
              else
                echo "âŒ CRITICAL: Edge Function fallback created file but it's too small ($FILE_SIZE bytes)"
                echo "âŒ First 20 lines:"
                head -n 20 backup/db/backup.sql
                exit 1
              fi
            else
              echo "âŒ CRITICAL: Database export file is empty or missing"
              echo "âŒ pg_dump failed and Edge Function fallback did not create a backup file"
              echo "âŒ Please check the Edge Function logs above for errors"
              exit 1
            fi
          fi
          
          set -e  # Re-enable exit on error after handling pg_dump

      - name: Export auth users and system_users
        continue-on-error: true  # âœ… Continue even if auth export fails (app uses custom auth)
        run: |
          mkdir -p backup/auth
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          if (!supabaseUrl || !supabaseKey) {
            console.error('âœ— Missing Supabase credentials');
            process.exit(1);
          }
          
          // âœ… FIX: Validate URL format
          if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
            console.error('âœ— Invalid SUPABASE_URL format');
            console.error('URL must start with http:// or https://');
            console.error('Current value preview:', supabaseUrl.substring(0, 50));
            console.error('Expected format: https://xxxxx.supabase.co');
            process.exit(1);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          const supabase = createClient(cleanUrl, supabaseKey);
          
          // âœ… FIX: Export system_users table data (custom auth)
          console.log('âœ“ Exporting system_users table (custom authentication)...');
          
          let supabaseAuthUsers = [];
          let systemUsers = [];
          
          // First, try to export Supabase Auth users (if any)
          supabase.auth.admin.listUsers()
            .then(({ data, error }) => {
              if (!error && data && data.users) {
                supabaseAuthUsers = data.users;
                console.log('âœ“ Found', data.users.length, 'Supabase Auth users');
              } else {
                console.log('â„¹ No Supabase Auth users (using custom auth)');
              }
              
              // Now export system_users table
              return supabase
                .from('system_users')
                .select('*')
                .order('created_at', { ascending: false });
            })
            .then(({ data: systemUsersData, error: systemUsersError }) => {
              if (systemUsersError) {
                console.error('âœ— Error fetching system_users:', systemUsersError.message);
                throw systemUsersError;
              }
              
              if (!systemUsersData || systemUsersData.length === 0) {
                console.log('â„¹ No users found in system_users table');
                systemUsers = [];
              } else {
                systemUsers = systemUsersData;
                console.log('âœ“ Found', systemUsersData.length, 'users in system_users table');
              }
              
              // âœ… FIX: Create users.json with both Supabase Auth users and system_users
              const usersExport = {
                exported_at: new Date().toISOString(),
                supabase_auth_users: supabaseAuthUsers,
                system_users: systemUsers,
                total_users: supabaseAuthUsers.length + systemUsers.length,
                note: systemUsers.length > 0 
                  ? 'Users exported from system_users table (custom authentication system)'
                  : 'No users found in system_users table'
              };
              
              fs.writeFileSync(
                'backup/auth/users.json',
                JSON.stringify(usersExport, null, 2)
              );
              
              console.log('âœ“ Exported users to backup/auth/users.json');
              console.log('  - Supabase Auth users:', supabaseAuthUsers.length);
              console.log('  - System users (system_users table):', systemUsers.length);
              console.log('  - Total users:', usersExport.total_users);
              
              process.exit(0);
            })
            .catch((err) => {
              // If Supabase Auth fails, still try to export system_users
              console.log('â„¹ Supabase Auth not accessible, exporting system_users only...');
              
              supabase
                .from('system_users')
                .select('*')
                .order('created_at', { ascending: false })
                .then(({ data: systemUsersData, error: systemUsersError }) => {
                  if (systemUsersError) {
                    console.error('âœ— Error fetching system_users:', systemUsersError.message);
                    throw systemUsersError;
                  }
                  
                  const systemUsers = systemUsersData || [];
                  console.log('âœ“ Found', systemUsers.length, 'users in system_users table');
                  
                  const usersExport = {
                    exported_at: new Date().toISOString(),
                    supabase_auth_users: [],
                    system_users: systemUsers,
                    total_users: systemUsers.length,
                    note: systemUsers.length > 0 
                      ? 'Users exported from system_users table (custom authentication system). Supabase Auth not accessible.'
                      : 'No users found in system_users table'
                  };
                  
                  fs.writeFileSync(
                    'backup/auth/users.json',
                    JSON.stringify(usersExport, null, 2)
                  );
                  
                  console.log('âœ“ Exported', systemUsers.length, 'system users to backup/auth/users.json');
                  process.exit(0);
                })
                .catch((finalErr) => {
                  console.error('âœ— Failed to export users:', finalErr.message);
                  
                  // Create a note file as fallback
                  fs.writeFileSync(
                    'backup/auth/users.json',
                    JSON.stringify({
                      exported_at: new Date().toISOString(),
                      supabase_auth_users: [],
                      system_users: [],
                      total_users: 0,
                      error: finalErr.message,
                      note: 'Failed to export users. Check database connection and permissions.'
                    }, null, 2)
                  );
                  console.log('âœ“ Created users.json with error note');
                  process.exit(0); // Don't fail the workflow
                });
            });
          "

      - name: Download Supabase Storage files
        continue-on-error: false  # âœ… Fail if storage backup fails (important data)
        run: |
          mkdir -p backup/storage
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');
          const path = require('path');
          
          (async () => {
            try {
              const supabaseUrl = process.env.SUPABASE_URL;
              const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
              
              // âœ… FIX: Auto-detect buckets if SUPABASE_BUCKETS_TO_BACKUP is not set
              let buckets = (process.env.SUPABASE_BUCKETS_TO_BACKUP || '')
                .split(',')
                .map(b => b.trim())
                .filter(Boolean);
              
              if (!supabaseUrl || !supabaseKey) {
                console.error('âœ— Missing Supabase credentials');
                process.exit(1);
              }
              
              // âœ… FIX: Validate URL format
              if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
                console.error('âœ— Invalid SUPABASE_URL format');
                console.error('URL must start with http:// or https://');
                console.error('Current value preview:', supabaseUrl.substring(0, 50));
                console.error('Expected format: https://xxxxx.supabase.co');
                process.exit(1);
              }
              
              const cleanUrl = supabaseUrl.replace(/\/$/, '');
              const supabase = createClient(cleanUrl, supabaseKey);
              
              // âœ… FIX: Auto-detect buckets if not specified
              if (buckets.length === 0) {
                console.log('â„¹ SUPABASE_BUCKETS_TO_BACKUP not set, auto-detecting buckets...');
                try {
                  const { data: allBuckets, error: listError } = await supabase.storage.listBuckets();
              if (listError) {
                console.error('âœ— Error listing buckets:', listError.message);
                throw listError;
              }
              
              if (allBuckets && allBuckets.length > 0) {
                buckets = allBuckets.map(b => b.id).filter(id => id !== 's3'); // Exclude 's3' bucket name (it's a metadata identifier)
                console.log('âœ“ Auto-detected buckets:', buckets.join(', '));
              } else {
                console.log('â„¹ No Supabase Storage buckets found');
                // Create a note about S3 storage
                fs.writeFileSync(
                  'backup/storage/note.txt',
                  'No Supabase Storage buckets found.\\n' +
                  'If your files are stored in AWS S3, they are not included in this backup.\\n' +
                  'S3 files should be backed up separately using AWS backup tools or S3 lifecycle policies.\\n' +
                  'File metadata (file_metadata table) is included in the database backup.'
                );
                console.log('âœ“ Created storage backup note');
                process.exit(0);
              }
            } catch (detectErr) {
              console.error('âœ— Error auto-detecting buckets:', detectErr.message);
              // Create a note file
              fs.writeFileSync(
                'backup/storage/note.txt',
                'Could not detect storage buckets.\\n' +
                'Error: ' + detectErr.message + '\\n' +
                'Set SUPABASE_BUCKETS_TO_BACKUP secret in GitHub Settings to specify buckets manually.\\n' +
                'Example: profile-pictures,contracts,inventory,employees,branding,payroll,assets,custody'
              );
              process.exit(0); // Don't fail the workflow
            }
          } else {
            console.log('âœ“ Using specified buckets:', buckets.join(', '));
          }
          
          async function downloadBucket(bucketName) {
            try {
              console.log('  â†’ Listing files in bucket:', bucketName);
              const { data: files, error } = await supabase.storage
                .from(bucketName)
                .list('', { limit: 1000, recursive: true });
              
              if (error) {
                console.error('  âœ— Error listing files in', bucketName, ':', error.message);
                return { bucket: bucketName, downloaded: 0, error: error.message };
              }
              
              if (!files || files.length === 0) {
                console.log('  â„¹ No files in bucket:', bucketName);
                return { bucket: bucketName, downloaded: 0 };
              }
              
              console.log('  â†’ Found', files.length, 'files in', bucketName);
              const bucketDir = path.join('backup/storage', bucketName);
              fs.mkdirSync(bucketDir, { recursive: true });
              
              let downloaded = 0;
              let failed = 0;
              
              for (const file of files) {
                // Skip directories (they don't have an id)
                if (!file.id || file.name.endsWith('/')) {
                  continue;
                }
                
                try {
                  const { data, error: downloadError } = await supabase.storage
                    .from(bucketName)
                    .download(file.name);
                  
                  if (downloadError) {
                    console.error('    âœ— Error downloading', file.name, ':', downloadError.message);
                    failed++;
                    continue;
                  }
                  
                  const filePath = path.join(bucketDir, file.name);
                  const dir = path.dirname(filePath);
                  fs.mkdirSync(dir, { recursive: true });
                  fs.writeFileSync(filePath, Buffer.from(await data.arrayBuffer()));
                  downloaded++;
                  
                  if (downloaded % 10 === 0) {
                    console.log('    â†’ Downloaded', downloaded, 'files...');
                  }
                } catch (fileErr) {
                  console.error('    âœ— Error processing', file.name, ':', fileErr.message);
                  failed++;
                }
              }
              
              console.log('  âœ“ Downloaded', downloaded, 'files from', bucketName, (failed > 0 ? '(failed: ' + failed + ')' : ''));
              return { bucket: bucketName, downloaded, failed };
            } catch (bucketErr) {
              console.error('  âœ— Error backing up bucket', bucketName, ':', bucketErr.message);
              return { bucket: bucketName, downloaded: 0, error: bucketErr.message };
            }
          }
          
          Promise.all(buckets.map(downloadBucket))
            .then((results) => {
              const totalDownloaded = results.reduce((sum, r) => sum + (r.downloaded || 0), 0);
              const totalFailed = results.reduce((sum, r) => sum + (r.failed || 0), 0);
              
              console.log('âœ“ Storage backup completed');
              console.log('  - Buckets processed:', buckets.length);
              console.log('  - Total files downloaded:', totalDownloaded);
              if (totalFailed > 0) {
                console.log('  âš ï¸ Failed downloads:', totalFailed);
              }
              
              // Check if any buckets had errors
              const bucketsWithErrors = results.filter(r => r.error);
              if (bucketsWithErrors.length > 0) {
                const errorBuckets = bucketsWithErrors.map(r => r.bucket).join(', ');
                const errorDetails = bucketsWithErrors.map(r => r.bucket + ' (' + r.error + ')').join(', ');
                console.error('  âœ— Errors in buckets:', errorBuckets);
                throw new Error('Failed to backup ' + bucketsWithErrors.length + ' bucket(s): ' + errorDetails);
              }
              
              // Create a summary file
              fs.writeFileSync(
                'backup/storage/backup_summary.json',
                JSON.stringify({
                  exported_at: new Date().toISOString(),
                  buckets_backed_up: buckets,
                  results: results,
                  total_files_downloaded: totalDownloaded,
                  total_failed: totalFailed,
                  note: 'Supabase Storage buckets backed up. S3 files are backed up in a separate step (backup/storage/s3/).'
                }, null, 2)
              );
              
              if (totalDownloaded === 0 && buckets.length > 0) {
                console.log('  â„¹ï¸ No files found in any buckets (this may be normal if buckets are empty)');
              }
            })
            .catch((err) => {
              console.error('âœ— Failed to download storage files:', err);
              console.error('âœ— Error details:', err.message);
              // Create error note
              fs.writeFileSync(
                'backup/storage/error_note.txt',
                'Storage backup failed: ' + err.message + '\\n' +
                'File metadata is still available in the database backup (file_metadata table).'
              );
              throw err; // Fail the workflow
            });
            } catch (err) {
              console.error('âœ— Failed to download storage files:', err);
              console.error('âœ— Error details:', err.message);
              fs.writeFileSync(
                'backup/storage/error_note.txt',
                'Storage backup failed: ' + err.message + '\\n' +
                'File metadata is still available in the database backup (file_metadata table).'
              );
              process.exit(1);
            }
          })();
          "

      - name: Download S3 Storage files
        continue-on-error: true  # âœ… Continue even if S3 backup fails
        run: |
          mkdir -p backup/storage/s3
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          const { S3Client, GetObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const path = require('path');
          
          (async () => {
            try {
              const supabaseUrl = process.env.SUPABASE_URL;
              const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
              const awsAccessKeyId = process.env.AWS_ACCESS_KEY_ID;
              const awsSecretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;
              const awsRegion = process.env.AWS_S3_REGION || 'us-east-1';
              const awsBucket = process.env.AWS_S3_BUCKET;
              const backupS3Files = process.env.BACKUP_S3_FILES !== 'false';
              
              if (!backupS3Files) {
                console.log('â„¹ S3 file backup is disabled (BACKUP_S3_FILES=false)');
                fs.writeFileSync('backup/storage/s3/note.txt', 'S3 file backup is disabled.\\nSet BACKUP_S3_FILES=true in GitHub Secrets to enable S3 file backup.');
                process.exit(0);
              }
              
              if (!supabaseUrl || !supabaseKey) {
                console.error('âœ— Missing Supabase credentials');
                process.exit(1);
              }
              
              if (!awsAccessKeyId || !awsSecretAccessKey || !awsBucket) {
                console.log('â„¹ AWS S3 credentials not configured');
                fs.writeFileSync('backup/storage/s3/note.txt', 'AWS S3 credentials not configured. File metadata is still included in the database backup.');
                process.exit(0);
              }
              
              if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
                console.error('âœ— Invalid SUPABASE_URL format');
                process.exit(1);
              }
              
              const cleanUrl = supabaseUrl.replace(/\/\$/, '');
              const supabase = createClient(cleanUrl, supabaseKey);
              const s3Client = new S3Client({
                region: awsRegion,
                credentials: { accessKeyId: awsAccessKeyId, secretAccessKey: awsSecretAccessKey },
              });
              
              console.log('âœ“ Fetching S3 file list from file_metadata table...');
              
              let offset = 0;
              const limit = 1000;
              let allFiles = [];
              let hasMore = true;
              
              while (hasMore) {
                const { data: files, error } = await supabase
                  .from('file_metadata')
                  .select('id, path, file_name, mime_type, size, category, owner_id, owner_type, created_at')
                  .eq('bucket', 's3')
                  .is('deleted_at', null)
                  .range(offset, offset + limit - 1)
                  .order('created_at', { ascending: false });
                
                if (error) {
                  console.error('âœ— Error fetching file metadata:', error.message);
                  throw error;
                }
                
                if (!files || files.length === 0) {
                  hasMore = false;
                  break;
                }
                
                allFiles = allFiles.concat(files);
                offset += limit;
                hasMore = files.length === limit;
              }
              
              if (allFiles.length === 0) {
                console.log('â„¹ No S3 files found in file_metadata table');
                fs.writeFileSync('backup/storage/s3/note.txt', 'No S3 files found in file_metadata table.');
                process.exit(0);
              }
              
              console.log('âœ“ Found', allFiles.length, 'S3 files to backup');
              let downloaded = 0;
              let failed = 0;
              const failedFiles = [];
              
              for (const file of allFiles) {
                try {
                  const getObjectCommand = new GetObjectCommand({ Bucket: awsBucket, Key: file.path });
                  const response = await s3Client.send(getObjectCommand);
                  const chunks = [];
                  for await (const chunk of response.Body) chunks.push(chunk);
                  const fileBuffer = Buffer.concat(chunks);
                  const backupPath = path.join('backup/storage/s3', file.path);
                  fs.mkdirSync(path.dirname(backupPath), { recursive: true });
                  fs.writeFileSync(backupPath, fileBuffer);
                  downloaded++;
                  if (downloaded % 10 === 0) console.log('  â†’ Downloaded', downloaded, '/', allFiles.length, 'files...');
                } catch (fileErr) {
                  failed++;
                  failedFiles.push({ path: file.path, file_name: file.file_name, error: fileErr.message });
                  console.error('  âœ— Failed to download', file.path, ':', fileErr.message);
                }
              }
              
              console.log('âœ“ S3 backup completed');
              console.log('  - Files downloaded:', downloaded, '/', allFiles.length);
              if (failed > 0) console.log('  - Failed downloads:', failed);
              
              fs.writeFileSync('backup/storage/s3/backup_summary.json', JSON.stringify({
                exported_at: new Date().toISOString(),
                total_files: allFiles.length,
                downloaded: downloaded,
                failed: failed,
                s3_bucket: awsBucket,
                s3_region: awsRegion,
                failed_files: failedFiles
              }, null, 2));
              
              if (failed > 0) {
                fs.writeFileSync('backup/storage/s3/failed_files.json', JSON.stringify(failedFiles, null, 2));
                console.log('  âš  Created failed_files.json with', failed, 'failed downloads');
              }
            } catch (err) {
              console.error('âœ— S3 backup failed:', err.message);
              fs.writeFileSync('backup/storage/s3/error_note.txt', 'S3 backup failed: ' + err.message);
              process.exit(0);
            }
          })();
          "

      - name: Create backup archive
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%d-%H-%M-UTC")
          BACKUP_NAME="backup-$TIMESTAMP.zip"
          zip -r "$BACKUP_NAME" backup/
          echo "BACKUP_FILE=$BACKUP_NAME" >> $GITHUB_ENV
          echo "BACKUP_SIZE=$(stat -f%z "$BACKUP_NAME" 2>/dev/null || stat -c%s "$BACKUP_NAME")" >> $GITHUB_ENV
          echo "âœ“ Created backup archive: $BACKUP_NAME"

      - name: Upload to S3
        # âœ… FIX: Track upload progress and verify completion
        run: |
          S3_PATH="backups/$(date -u +"%Y/%m/%d")/${{ env.BACKUP_FILE }}"
          
          echo "ðŸ“¤ Starting S3 upload..."
          echo "  Source: ${{ env.BACKUP_FILE }}"
          echo "  Destination: s3://$AWS_S3_BUCKET/$S3_PATH"
          echo "  Size: $(du -h "${{ env.BACKUP_FILE }}" | cut -f1)"
          
          # âœ… FIX: Upload with progress tracking
          aws s3 cp "${{ env.BACKUP_FILE }}" "s3://$AWS_S3_BUCKET/$S3_PATH" \
            --region "$AWS_S3_REGION" \
            --no-progress 2>&1 | while IFS= read -r line; do
              echo "$line"
              # Track upload progress if available
              if echo "$line" | grep -q "upload:"; then
                echo "  â†’ Upload in progress..."
              fi
            done
          
          # âœ… FIX: Verify upload completed successfully
          echo "ðŸ” Verifying S3 upload..."
          if aws s3 ls "s3://$AWS_S3_BUCKET/$S3_PATH" --region "$AWS_S3_REGION" > /dev/null 2>&1; then
            UPLOADED_SIZE=$(aws s3 ls "s3://$AWS_S3_BUCKET/$S3_PATH" --region "$AWS_S3_REGION" --human-readable --summarize | grep "Total Size" | awk '{print $3, $4}')
            echo "âœ“ Upload verified successfully"
            echo "âœ“ Uploaded file size: $UPLOADED_SIZE"
            echo "S3_KEY=$S3_PATH" >> $GITHUB_ENV
            echo "UPLOAD_COMPLETED=true" >> $GITHUB_ENV
          else
            echo "âŒ ERROR: Upload verification failed - file not found in S3"
            echo "UPLOAD_COMPLETED=false" >> $GITHUB_ENV
            exit 1
          fi

      - name: Update backup_history via Edge Function
        # âœ… NEW: Use update-backup edge function instead of direct Supabase update
        # This ensures proper validation, idempotency, and error handling
        # Falls back to direct Supabase update if edge function fails
        # Note: Environment variables are inherited from job-level env
        run: |
          # âœ… FIX: Check if upload completed
          if [ "${{ env.UPLOAD_COMPLETED }}" != "true" ]; then
            echo "âŒ ERROR: S3 upload did not complete successfully"
            echo "âŒ Cannot update backup_history - upload verification failed"
            exit 1
          fi
          
          # Get variables from environment (set at job level or from previous steps)
          S3_KEY="${{ env.S3_KEY }}"
          BACKUP_SIZE="${{ env.BACKUP_SIZE }}"
          DISPATCH_ID="$DISPATCH_ID"
          WORKFLOW_RUN_ID="${{ github.run_id }}"
          
          # These are available from job-level env
          # SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY are already set
          # BACKUP_API_KEY is optional (from job-level env if set)
          
          # Get backup_id from dispatch_id using Supabase REST API (no Node.js module needed)
          echo "ðŸ” Finding backup_id for dispatch_id: $DISPATCH_ID"
          
          # Use Supabase REST API directly via curl
          SUPABASE_REST_URL="${SUPABASE_URL}/rest/v1/backup_history"
          BACKUP_QUERY_RESPONSE=$(curl -s -X GET \
            "${SUPABASE_REST_URL}?dispatch_id=eq.${DISPATCH_ID}&select=id" \
            -H "apikey: ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Content-Type: application/json" \
            -H "Prefer: return=representation")
          
          echo "ðŸ“¡ Query response: $BACKUP_QUERY_RESPONSE"
          
          # Extract backup_id from JSON response using jq or grep/sed
          if command -v jq &> /dev/null; then
            BACKUP_ID=$(echo "$BACKUP_QUERY_RESPONSE" | jq -r '.[0].id // empty')
          else
            # Fallback: extract ID using grep/sed
            BACKUP_ID=$(echo "$BACKUP_QUERY_RESPONSE" | grep -o '"id":"[^"]*"' | head -1 | sed 's/"id":"\([^"]*\)"/\1/')
          fi
          
          # If still empty, try alternative extraction
          if [ -z "$BACKUP_ID" ] || [ "$BACKUP_ID" = "null" ]; then
            # Try extracting UUID pattern directly
            BACKUP_ID=$(echo "$BACKUP_QUERY_RESPONSE" | grep -oE '[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}' | head -1)
          fi
          
          if [ -z "$BACKUP_ID" ] || [ "$BACKUP_ID" = "" ]; then
            echo "âŒ ERROR: Could not find backup_id for dispatch_id: $DISPATCH_ID"
            echo "âŒ This means the backup entry was not created in backup_history"
            exit 1
          fi
          
          echo "âœ“ Found backup_id: $BACKUP_ID"
          echo "ðŸ“¤ Preparing to update backup_history..."
          echo "  - backup_id: $BACKUP_ID"
          echo "  - s3_key: $S3_KEY"
          echo "  - size_bytes: $BACKUP_SIZE"
          echo "  - workflow_run_id: $WORKFLOW_RUN_ID"
          
          # Check if BACKUP_API_KEY is set
          if [ -z "$BACKUP_API_KEY" ] || [ "$BACKUP_API_KEY" = "" ]; then
            echo "âš  WARNING: BACKUP_API_KEY not set, using direct Supabase update (fallback)"
            USE_EDGE_FUNCTION=false
          else
            echo "âœ“ BACKUP_API_KEY is set, attempting edge function call"
            USE_EDGE_FUNCTION=true
          fi
          
          # Try edge function first if API key is available
          if [ "$USE_EDGE_FUNCTION" = "true" ]; then
            echo "ðŸ“¡ Calling edge function: $SUPABASE_URL/functions/v1/update-backup"
            
            RESPONSE=$(curl -s -w "\n%{http_code}" -X POST "$SUPABASE_URL/functions/v1/update-backup" \
              -H "Authorization: Bearer $BACKUP_API_KEY" \
              -H "Content-Type: application/json" \
              -d "{
                \"backup_id\": \"$BACKUP_ID\",
                \"s3_key\": \"$S3_KEY\",
                \"size_bytes\": $BACKUP_SIZE,
                \"workflow_run_id\": \"$WORKFLOW_RUN_ID\"
              }" 2>&1)
            
            HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
            BODY=$(echo "$RESPONSE" | sed '$d')
            
            echo "ðŸ“¡ Edge function response HTTP Code: $HTTP_CODE"
            echo "ðŸ“¡ Edge function response Body: $BODY"
            
            if [ "$HTTP_CODE" -eq 200 ]; then
              echo "âœ“ Backup history updated successfully via edge function"
              echo "$BODY" | jq '.' 2>/dev/null || echo "$BODY"
              EDGE_FUNCTION_SUCCESS=true
            else
              echo "âš  WARNING: Edge function call failed (HTTP $HTTP_CODE)"
              echo "âš  Response: $BODY"
              EDGE_FUNCTION_SUCCESS=false
            fi
          else
            EDGE_FUNCTION_SUCCESS=false
          fi
          
          # Fallback to direct Supabase update if edge function failed or not available
          if [ "$EDGE_FUNCTION_SUCCESS" != "true" ]; then
            echo "ðŸ“ Using direct Supabase REST API update (fallback method)..."
            
            # Prepare update payload using printf to avoid YAML parsing issues
            FINISHED_AT=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
            UPDATE_PAYLOAD="{"
            UPDATE_PAYLOAD="${UPDATE_PAYLOAD}\"s3_key\":\"${S3_KEY}\","
            UPDATE_PAYLOAD="${UPDATE_PAYLOAD}\"status\":\"success\","
            UPDATE_PAYLOAD="${UPDATE_PAYLOAD}\"finished_at\":\"${FINISHED_AT}\","
            UPDATE_PAYLOAD="${UPDATE_PAYLOAD}\"error_text\":null"
            if [ -n "$BACKUP_SIZE" ] && [ "$BACKUP_SIZE" != "" ]; then
              UPDATE_PAYLOAD="${UPDATE_PAYLOAD},\"size_bytes\":${BACKUP_SIZE}"
            fi
            if [ -n "$WORKFLOW_RUN_ID" ]; then
              UPDATE_PAYLOAD="${UPDATE_PAYLOAD},\"workflow_run_id\":\"${WORKFLOW_RUN_ID}\""
            fi
            UPDATE_PAYLOAD="${UPDATE_PAYLOAD}}"
            
            echo "ðŸ“¤ Update payload:"
            echo "$UPDATE_PAYLOAD" | jq '.' 2>/dev/null || echo "$UPDATE_PAYLOAD"
            
            # Update via Supabase REST API
            SUPABASE_UPDATE_URL="${SUPABASE_URL}/rest/v1/backup_history"
            UPDATE_RESPONSE=$(curl -s -w "\n%{http_code}" -X PATCH \
              "${SUPABASE_UPDATE_URL}?id=eq.${BACKUP_ID}" \
              -H "apikey: ${SUPABASE_SERVICE_ROLE_KEY}" \
              -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
              -H "Content-Type: application/json" \
              -H "Prefer: return=representation" \
              -d "$UPDATE_PAYLOAD")
            
            UPDATE_HTTP_CODE=$(echo "$UPDATE_RESPONSE" | tail -n1)
            UPDATE_BODY=$(echo "$UPDATE_RESPONSE" | sed '$d')
            
            echo "ðŸ“¡ Fallback update HTTP Code: $UPDATE_HTTP_CODE"
            echo "ðŸ“¡ Fallback update Response: $UPDATE_BODY"
            
            if [ "$UPDATE_HTTP_CODE" -ge 200 ] && [ "$UPDATE_HTTP_CODE" -lt 300 ]; then
              echo "âœ“ Backup history updated via fallback method (REST API)"
              echo "$UPDATE_BODY" | jq '.' 2>/dev/null || echo "$UPDATE_BODY"
            else
              echo "âŒ ERROR: Fallback update failed (HTTP $UPDATE_HTTP_CODE)"
              echo "âŒ Response: $UPDATE_BODY"
              echo "âŒ This is a critical error - backup file is in S3 but database was not updated"
              echo "âŒ Manual intervention required to update backup_history table"
              exit 1
            fi
          fi
          
          # Update system_settings_kv (keep existing logic)
          echo "ðŸ“ Updating system_settings..."
          node --input-type=commonjs -e "
          const { createClient } = require('@supabase/supabase-js');
          
          const supabaseUrl = process.env.SUPABASE_URL;
          const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
          
          if (!supabaseUrl || !supabaseKey) {
            console.log('âš  Skipping system_settings update - missing credentials');
            process.exit(0);
          }
          
          const cleanUrl = supabaseUrl.replace(/\/$/, '');
          const supabase = createClient(cleanUrl, supabaseKey);
              
              // Update system_settings_kv
              // Note: Service role key should bypass RLS, but RLS policy requires auth.role() = 'service_role'
              // When using service_role key directly, we need to ensure it's recognized
              // If this fails, it's not critical - the backup itself succeeded
          supabase
                .from('system_settings_kv')
                .upsert({
                  key: 'last_backup_at',
                  value: new Date().toISOString(),
                }, {
                  onConflict: 'key',
                })
            .select()
            .then(({ data: settingsData, error: settingsError }) => {
              if (settingsError) {
                console.error('âš  Warning: Failed to update last_backup_at:', settingsError.message);
                console.error('âš  Error code:', settingsError.code);
                console.error('âš  Error details:', settingsError.details);
                console.error('âš  This is not critical - backup completed successfully');
                console.error('âš  The RLS policy might be blocking service_role updates');
                console.error('âš  To fix: Update the RLS policy to allow service_role, or disable RLS for this table');
                // Don't fail the step, just log the warning
              } else {
                console.log('âœ“ System settings updated');
                if (settingsData) {
                  console.log('âœ“ Updated last_backup_at timestamp');
                }
              }
            })
            .catch((err) => {
              console.error('âš  Warning: Error updating system settings:', err.message);
              // Don't fail - backup update already succeeded
            });
          "
        env:
          SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
          GITHUB_RUN_ID: ${{ github.run_id }}
          BACKUP_API_KEY: ${{ secrets.BACKUP_API_KEY }}

      - name: Handle backup failure
        if: failure()
        # âœ… FIX: Use the same secret names as job-level env
        env:
          SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.VITE_SUPABASE_SERVICE_ROLE_KEY }}
          DISPATCH_ID: ${{ github.event.inputs.dispatch_id || github.run_id }}
        run: |
          FINAL_STATUS="failed"
          FINAL_ERROR_TEXT="Backup workflow failed. Check GitHub Actions logs for details."
          
          # Debug: Check if variables are available
          echo "Debug: Checking environment variables in failure handler..."
          echo "SUPABASE_URL: ${SUPABASE_URL:+SET (hidden)}${SUPABASE_URL:-MISSING}"
          echo "SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY:+SET (hidden)}${SUPABASE_SERVICE_ROLE_KEY:-MISSING}"
          echo "DISPATCH_ID: $DISPATCH_ID"
          
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_SERVICE_ROLE_KEY" ] || [ -z "$DISPATCH_ID" ]; then
            echo "âŒ Cannot update backup status - missing required variables"
            echo "This is a critical error. Please check GitHub Secrets configuration."
            exit 1
          fi
          
          # âœ… FIX: Install dependencies if not already installed (failure handler runs even if earlier steps fail)
          USE_FETCH_FALLBACK=false
          if [ ! -d "node_modules/@supabase" ]; then
            echo "Installing @supabase/supabase-js..."
            if ! npm install @supabase/supabase-js --no-save 2>/dev/null; then
              echo "âš  Failed to install @supabase/supabase-js, will use fetch API fallback..."
              USE_FETCH_FALLBACK=true
            fi
          fi
          
          # âœ… FIX: Use fetch API if npm install failed or module still not available
          if [ "$USE_FETCH_FALLBACK" = "true" ] || [ ! -d "node_modules/@supabase" ]; then
            echo "Using fetch API to update backup status..."
            STATUS="$FINAL_STATUS" \
            ERROR_TEXT="$FINAL_ERROR_TEXT" \
            node --input-type=commonjs -e "
            (async () => {
              try {
                const supabaseUrl = (process.env.SUPABASE_URL || '').replace(/\\/$/, '');
                const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
                const dispatchId = process.env.DISPATCH_ID;
                const status = process.env.STATUS;
                const errorText = process.env.ERROR_TEXT;
                
                if (!supabaseUrl || !supabaseKey || !dispatchId) {
                  console.error('âœ— Missing required variables for fetch API fallback');
                  process.exit(1);
                }
                
                // Use direct REST API call instead of supabase-js
                const response = await fetch(supabaseUrl + '/rest/v1/backup_history?dispatch_id=eq.' + encodeURIComponent(dispatchId), {
                  method: 'PATCH',
                  headers: {
                    'apikey': supabaseKey,
                    'Authorization': 'Bearer ' + supabaseKey,
                    'Content-Type': 'application/json',
                    'Prefer': 'return=minimal'
                  },
                  body: JSON.stringify({
                    status: status,
                    error_text: errorText
                  })
                });
                
                if (response.ok) {
                  console.log('âœ“ Backup status updated to failed via REST API');
                } else {
                  const errorText = await response.text();
                  console.error('âœ— Failed to update backup status:', errorText);
                  process.exit(1);
                }
              } catch (err) {
                console.error('âœ— Error updating backup status:', err.message);
                process.exit(1);
              }
            })();
            "
            exit 0
          fi
          
          STATUS="$FINAL_STATUS" \
          ERROR_TEXT="$FINAL_ERROR_TEXT" \
          node --input-type=commonjs -e "
          (async () => {
            try {
              const { createClient } = require('@supabase/supabase-js');
              
              const supabaseUrl = process.env.SUPABASE_URL;
              const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
              const dispatchId = process.env.DISPATCH_ID;
              const status = process.env.STATUS;
              const errorText = process.env.ERROR_TEXT;
              
              if (!supabaseUrl || !supabaseKey || !dispatchId) {
                console.error('âœ— Cannot update backup status - missing credentials');
                console.error('SUPABASE_URL:', supabaseUrl ? 'SET' : 'MISSING');
                console.error('SUPABASE_SERVICE_ROLE_KEY:', supabaseKey ? 'SET' : 'MISSING');
                console.error('DISPATCH_ID:', dispatchId ? 'SET' : 'MISSING');
                process.exit(1);
              }
              
              // âœ… FIX: Validate URL format
              if (!supabaseUrl.startsWith('http://') && !supabaseUrl.startsWith('https://')) {
                console.error('âœ— Invalid SUPABASE_URL format');
                console.error('URL must start with http:// or https://');
                console.error('Current value preview:', supabaseUrl.substring(0, 50));
                console.error('Expected format: https://xxxxx.supabase.co');
                process.exit(1);
              }
              
              // âœ… FIX: Remove trailing slash if present
              const cleanUrl = supabaseUrl.replace(/\\/$/, '');
              
              console.log('âœ“ Updating backup status to failed');
              const supabase = createClient(cleanUrl, supabaseKey);
              
              const { error } = await supabase
                .from('backup_history')
                .update({
                  status: status,
                  error_text: errorText,
                })
                .eq('dispatch_id', dispatchId);
              
              if (error) {
                console.error('âœ— Failed to update backup status to failed:', error.message);
                process.exit(1);
              } else {
                console.log('âœ“ Backup status updated to failed');
              }
            } catch (err) {
              console.error('âœ— Error updating backup status:', err);
              process.exit(1);
            }
          })();
          "
